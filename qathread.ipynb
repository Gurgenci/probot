{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from ute import init_openai, read_page_list, rdweb, embedpagelist, sepstr\n",
    "from ute import SEP, get_embedding, cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "(Client, LLM)=init_openai(model=\"gpt-4-1106-preview\")\n",
    "webfolder=\"general\"\n",
    "datafolder=\"data\"+\"/\"+webfolder+\"/\"\n",
    "DataFile=datafolder+\"data\"+\".txt\"\n",
    "TextListFile=datafolder+webfolder+\"_textlist.txt\"\n",
    "UploadListFile=datafolder+webfolder+\"_uploadlist.txt\"\n",
    "UploadIDFile=datafolder+webfolder+\"_uploadid.txt\"\n",
    "EmbeddingFile=datafolder+webfolder+\"_embedding.npz\"\n",
    "#\n",
    "TextSimilarity=None\n",
    "topN=None\n",
    "N=3\n",
    "#\n",
    "Thread=None\n",
    "Run=None\n",
    "Messages=[]\n",
    "RunIDs=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time stamp to local time\n",
    "def ts2lt(ts):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run globals.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 12px;'>\n",
       "\n",
       "|Contents|File Name|\n",
       "|--|--|\n",
       "|Web folder name|general|\n",
       "|Data folder name|data/general/|\n",
       "|Data file name|data/general/data.txt|\n",
       "|Text list file name|data/general/general_textlist.txt|\n",
       "|Upload list file name|data/general/general_uploadlist.txt|\n",
       "|Upload ID file name|data/general/general_uploadid.txt|\n",
       "|Embedding file name|data/general/general_embedding.npz|\n",
       "|Client|<openai.OpenAI object at 0x108b04810>|\n",
       "|LLM|text-embedding-ada-002|\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "|Variable name|Description|\n",
       "|--|--|\n",
       "|`webfolder`|Web folder name|\n",
       "|`datafolder`|Data folder name|\n",
       "|`DataFile`|Data file name|\n",
       "|`TextListFile`|Text list file name|\n",
       "|`UploadListFile`|Upload list file name|\n",
       "|`UploadIDFile`|Upload ID file name|\n",
       "|`EmbeddingFile`|Embedding file name|\n",
       "|`Client`|OpenAI client|\n",
       "|`LLM`|Language model|\n",
       "`UploadList`|List of file names to upload|\n",
       "`UploadID`|List of OpenAI file IDs (created when files are uploaded)|\n",
       "`TextList`|List of text files (as pairs of two lines, URL and title)|\n",
       "`Embedding`|Embedding matrix|\n",
       "`TextSimilarity`|Similarity vector|\n",
       "`topN`|`TextSimilarity` indices of the N best-matching pages|\n",
       "`N`|Number of best-matching pages to display|\n",
       "`Thread`|Current thread object|\n",
       "`Run`|Current run object|\n",
       "`Messages`|List of messages on the current thread|\n",
       "`RunIDs`|List of run IDs on the current thread|\n",
       "\n",
       "\n",
       "</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s=showglobals()\n",
    "mdc(s,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the text files locally and upload to OpenAI ##\n",
    "We upload the files to the file repository of the Client. They are not yet associated with any Thread nor Assistant.  We will initiate the Assistant next and a new thread is created after the Query is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the-requiem-for-a-dream-israels-untaken.txt because it has already been uploaded\n",
      "Skipping the-importance-of-elite-consensus.txt because it has already been uploaded\n",
      "Skipping openai-astounds-us-again.txt because it has already been uploaded\n",
      "Skipping why-did-elon-musk-buy-twitter.txt because it has already been uploaded\n",
      "Skipping elon-musk-the-spaceman.txt because it has already been uploaded\n",
      "Skipping let-us-talk-about-elon-musk.txt because it has already been uploaded\n",
      "Skipping despicable-acts-part-2.txt because it has already been uploaded\n",
      "Skipping despicable-deeds.txt because it has already been uploaded\n",
      "Skipping rogue-age-and-climate-change-unpredictable.txt because it has already been uploaded\n",
      "Skipping rogue-age-accessory-1-population.txt because it has already been uploaded\n",
      "Skipping rogue-renaissance-on-globe-with-upheavals.txt because it has already been uploaded\n",
      "Skipping the-great-stagnation-ends-but-for.txt because it has already been uploaded\n",
      "Skipping when-the-rivers-run-dry.txt because it has already been uploaded\n",
      "Skipping the-voice-referendum-in-australia.txt because it has already been uploaded\n",
      "Skipping conspiracy-theories-part-2.txt because it has already been uploaded\n",
      "Skipping conspiracy-theories-part-1.txt because it has already been uploaded\n",
      "Skipping how-many-more-ruins-in-great-britain.txt because it has already been uploaded\n",
      "Skipping while-watching-utopia-on-abc.txt because it has already been uploaded\n",
      "Skipping lying-oracles-and-the-anyone-and.txt because it has already been uploaded\n",
      "Skipping the-one-thing-necessary-for-the-triumph.txt because it has already been uploaded\n",
      "Skipping will-there-be-a-war.txt because it has already been uploaded\n",
      "Skipping how-do-we-select-our-information.txt because it has already been uploaded\n",
      "Skipping do-we-know-why-we-know-what-we-know.txt because it has already been uploaded\n",
      "Skipping does-layering-affect-heat-loss.txt because it has already been uploaded\n",
      "Skipping elections-in-turkey.txt because it has already been uploaded\n",
      "Skipping monster-wave-soon-to-hit-your-shore.txt because it has already been uploaded\n",
      "Skipping try-counting-carbons-not-calories.txt because it has already been uploaded\n",
      "Skipping bridge-fixture-for-five-players.txt because it has already been uploaded\n",
      "Skipping the-attainment-of-happiness.txt because it has already been uploaded\n",
      "Skipping competent-intelligence-is-here-will.txt because it has already been uploaded\n",
      "Skipping the-prodigal-bird-returns.txt because it has already been uploaded\n",
      "Skipping earthquake-thoughts-and-facts.txt because it has already been uploaded\n",
      "Skipping why-were-so-many-buildings-destroyed.txt because it has already been uploaded\n",
      "Skipping retirement-a-journey-of-continued.txt because it has already been uploaded\n",
      "No new files uploaded\n",
      "No new IDs uploaded\n"
     ]
    }
   ],
   "source": [
    "# The following function reads the names of the URLs already uploaded to OpenAI\n",
    "def read_upload_list(UploadListFile):\n",
    "    UploadList=[]\n",
    "    if os.path.exists(UploadListFile):\n",
    "        with open(UploadListFile, 'r') as f:\n",
    "            UploadList = f.read().splitlines()\n",
    "    return UploadList\n",
    "\n",
    "UploadList=read_upload_list(UploadListFile)\n",
    "\n",
    "# The following function reads the IDs of the URLs already uploaded to OpenAI\n",
    "def read_upload_id(UploadIDFile):\n",
    "    UploadID=[]\n",
    "    if os.path.exists(UploadIDFile):\n",
    "        with open(UploadIDFile, 'r') as f:\n",
    "            UploadID = f.read().splitlines()\n",
    "    return UploadID\n",
    "\n",
    "UploadID=read_upload_id(UploadIDFile)\n",
    "#\n",
    "# The following function extracts a file name from the URL.  The file name is\n",
    "# used to name the text file that will be uploaded to OpenAI\n",
    "# The file name is also used to check if the file has already been uploaded\n",
    "# Use the last part of the URL as the file name\n",
    "def get_file_name(url):\n",
    "    url_parts=url.split(\"/\")\n",
    "    filename=url_parts[-1]\n",
    "    return filename\n",
    "\n",
    "# Read the URL addresses to be read into text files from the file TextListFile\n",
    "# Ignore the URLs that have already been uploaded to OpenAI\n",
    "\n",
    "TextList=read_page_list(TextListFile)\n",
    "NewUploads=[]\n",
    "NewUploadIDs=[]\n",
    "for (itext,pair) in enumerate(TextList):\n",
    "    (url,title)=pair\n",
    "    txtfile=get_file_name(url)+\".txt\"\n",
    "    if txtfile in UploadList:\n",
    "        print(\"Skipping %s because it has already been uploaded\"%txtfile)\n",
    "        continue\n",
    "    # Read the text from the URL address and save into the text file\n",
    "    rdweb(url, datafolder+txtfile)\n",
    "    print(\"Uploading %s\"%txtfile, end=\"\")\n",
    "    File = Client.files.create(\n",
    "        file=open(datafolder+txtfile, \"rb\"),\n",
    "        purpose='assistants'\n",
    ")    \n",
    "    print(\" --- Uploaded\")\n",
    "    time.sleep(1)\n",
    "    NewUploads.append(txtfile)\n",
    "    NewUploadIDs.append(File.id)\n",
    "with open(UploadListFile, 'a') as f:\n",
    "    for item in NewUploads:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "if len(NewUploads)>0:\n",
    "    print(\"Updated %s (Names of the Uploaded Files)\"%UploadListFile)\n",
    "else:\n",
    "    print(\"No new files uploaded\")\n",
    "# Append the new IDs to the file UploadIDFile\n",
    "with open(UploadIDFile, 'a') as f:\n",
    "    for item in NewUploadIDs:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "if len(NewUploadIDs)>0:\n",
    "    print(\"Updated %s (OpenAI IDs of the uploaded files)\"%UploadIDFile)\n",
    "else:\n",
    "    print(\"No new IDs uploaded\")\n",
    "#\n",
    "UploadID=UploadID+NewUploadIDs\n",
    "UploadList=UploadList+NewUploads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Assistant ID from file or create new ##\n",
    "Delete `data/general/data.txt` if you want to create a new assistant.  This code does not check if an assistant with that name already exists assigned to your account in OpenAI.  You can see the assistants assigned to you in [Assistants Page](https://platform.openai.com/assistants) after you log in to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Assistant with ID from OpenAI API: asst_YfdA8UsATfJCHBZICrMV84aF\n"
     ]
    }
   ],
   "source": [
    "# Check if the DataFile exists\n",
    "if os.path.isfile(DataFile):\n",
    "    # Read the Assistant ID from the file\n",
    "    f = open(DataFile, \"r\")\n",
    "    AssistantID = f.read()\n",
    "    f.close()\n",
    "    Assistant = Client.beta.assistants.retrieve(AssistantID)\n",
    "    print(\"Retrieved Assistant with ID from OpenAI API: %s\"%AssistantID)\n",
    "else:\n",
    "    # Create a new Assistant\n",
    "    Assistant = Client.beta.assistants.create(\n",
    "    name=\"Assistant for \"+webfolder,\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    instructions=\"You are a friendly AI chatbot. You answer queries using the information in the text files attached to the query message.  If the \\\n",
    " response is not in the text files, you can respond with 'I don't know'.  Try to be informative with your answers.\",\n",
    "    tools=[{\"type\": \"retrieval\"}],\n",
    "    # file_ids=UploadID\n",
    "    )\n",
    "    AssistantID = Assistant.id\n",
    "    # Write the Assistant ID to the file\n",
    "    f = open(DataFile, \"w\")\n",
    "    f.write(AssistantID)\n",
    "    f.close()\n",
    "    print(\"Created Assistant ID: %s\"%AssistantID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings ##\n",
    "The OpenAI does not allow to attach more than 20 files.  I have more than 20 \n",
    "blog posts that I want to be able to query.  I do not want to combine them together.  Since I cannot attach more than 20 to an Assistant, I will have to find the files that are most relevant to the question and attach only those files.  These files can be selected in a number of ways. Before we consider different options, we need to generate embeddings.  In the following cell, I do the following:\n",
    "* Load embeddings from the `npz` file if it exists\n",
    "* If there is no `npz` file, embed all files and store with metadata\n",
    "* Check if there are new text files in the TextList\n",
    "* Embed the new text files and append them to the end of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new texts to be embedded\n"
     ]
    }
   ],
   "source": [
    "def embedandsave():\n",
    "    embeddings=embedpagelist(TextList)\n",
    "    metadata={\"model\":LLM, \"webfolder\":webfolder, \"textlist\":TextListFile, \"texts\":TextList}\n",
    "    np.savez(EmbeddingFile,data=embeddings, metadata=metadata)\n",
    "    return embeddings, metadata\n",
    "\n",
    "if not os.path.exists(EmbeddingFile):\n",
    "    (embeddings, metadata)=embedandsave()\n",
    "    print(\"Created %s\"%EmbeddingFile)\n",
    "# Load embeddings from file\n",
    "with np.load(EmbeddingFile, allow_pickle=True) as data:\n",
    "    embeddings=data['data']\n",
    "    metadata=data['metadata'].item()\n",
    "\n",
    "if not metadata[\"model\"]==LLM:\n",
    "    (embeddings, metadata)=embedandsave()\n",
    "    print(\"Re-created %s because of the LLM change\"%EmbeddingFile)   \n",
    "\n",
    "# Generate the liost of embedded textx that are in the npz file:\n",
    "embedded_texts=[]\n",
    "for pair in metadata[\"texts\"]:\n",
    "    embedded_texts.append(pair[1])\n",
    "\n",
    "# Now check if there are any new texts to be embedded\n",
    "update=False\n",
    "for (itext,pair) in enumerate(TextList):\n",
    "    url=pair[0]\n",
    "    texttitle=pair[1]\n",
    "    # print(\"Check '\"+texttitle+\"' --> \", end=\"\")\n",
    "    if texttitle not in embedded_texts:\n",
    "        print(\"Update \"+texttitle)\n",
    "        text=rdweb(url, None)\n",
    "        sa=sepstr(text, SEP)\n",
    "        for (isegment, s) in enumerate(sa):\n",
    "            v=get_embedding(s)\n",
    "            w=np.append(v,[itext,isegment])\n",
    "            embeddings=np.vstack((embeddings,w))\n",
    "        metadata[\"texts\"].append(pair)\n",
    "        update=True\n",
    "    # else:\n",
    "        # print(\"OK\")\n",
    "if update:\n",
    "    # Save the embeddings to a file\n",
    "    np.savez(EmbeddingFile,data=embeddings, metadata=metadata)\n",
    "    print(\"Updated %s\"%EmbeddingFile)\n",
    "else:\n",
    "    print(\"No new texts to be embedded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Questions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpMode=0\n",
    "OpTask=[\"check\", \"answer\", \"summarise\", \"quit\"][OpMode]\n",
    "\n",
    "def checkmode():\n",
    "    global OpMode\n",
    "    OpMode=0\n",
    "    print(\"Switched to check mode\")\n",
    "    return\n",
    "def answermode():\n",
    "    global OpMode\n",
    "    OpMode=1\n",
    "    print(\"Switched to answer mode\")\n",
    "    return\n",
    "def summarisemode():\n",
    "    global OpMode\n",
    "    OpMode=2\n",
    "    print(\"Switched to summarise mode\")\n",
    "    return\n",
    "def quitmode():\n",
    "    global OpMode\n",
    "    OpMode=3\n",
    "    print(\"Quit\")\n",
    "    return\n",
    "def setoptask():\n",
    "    global OpTask\n",
    "    OpTask=[\"check\", \"answer\", \"summarise\", \"quit\"][OpMode]\n",
    "    return\n",
    "\n",
    "Specials=[\"Check\", \"Answer\", \"Summarise\", \"Quit\"]\n",
    "SpecialFunctions=[checkmode, answermode, summarisemode, quitmode]\n",
    "SpecialEmbeds=[get_embedding(s) for s in Specials]\n",
    "OpMode=0\n",
    "OpTask=[\"check\", \"answer\", \"summarise\", \"quit\"][OpMode]\n",
    "\n",
    "def checkmode():\n",
    "    global OpMode\n",
    "    OpMode=0\n",
    "    print(\"Switched to check mode\")\n",
    "    return\n",
    "def answermode():\n",
    "    global OpMode\n",
    "    OpMode=1\n",
    "    print(\"Switched to answer mode\")\n",
    "    return\n",
    "def summarisemode():\n",
    "    global OpMode\n",
    "    OpMode=2\n",
    "    print(\"Switched to summarise mode\")\n",
    "    return\n",
    "def quitmode():\n",
    "    global OpMode\n",
    "    OpMode=3\n",
    "    print(\"Quit\")\n",
    "    return\n",
    "def setoptask():\n",
    "    global OpTask\n",
    "    OpTask=[\"check\", \"answer\", \"summarise\", \"quit\"][OpMode]\n",
    "    return\n",
    "\n",
    "Specials=[\"Check\", \"Answer\", \"Summarise\", \"Quit\"]\n",
    "SpecialFunctions=[checkmode, answermode, summarisemode, quitmode]\n",
    "SpecialEmbeds=[get_embedding(s) for s in Specials]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt ##\n",
    "In the thread, I will use the Assistant instructions that are already set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging our session ##\n",
    "I will store the query, the associated files, and the OpenAI response in a file.  This file will be in markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logging=True\n",
    "\n",
    "import datetime\n",
    "today = datetime.date.today()\n",
    "#\n",
    "if Logging:\n",
    "    Logfile=datafolder+webfolder+\"_log.md\"\n",
    "    Log=open(Logfile, \"a\")\n",
    "    Log.write(today.strftime(\"%B %d, %Y\")+\"\\n\\n\")\n",
    "    Log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(s):\n",
    "    if Logging:\n",
    "        Log=open(Logfile, \"a\")\n",
    "        Log.write(s+\"\\n\\n\")\n",
    "        Log.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation Mode ##\n",
    "This can do one of the following things depending on the value of the `OpMode`.  \n",
    "\n",
    "|Mode|Action|\n",
    "|:--:|----|\n",
    "|0 - Check|Do not answer. Only return the URLs of the pages that match |\n",
    "|1 - Answer|Find the best-match URLs, ask OpenAI to reply using thuse URL files|\n",
    "|2 - Summarise|Find the best-match URLs, ask OpenAI to summarise them| \n",
    "\n",
    "The default mode is `check`.  One can change the mode by entering one of the following strings (or similar) as the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|Query text|`OpTask`|\n",
       "|--|--|\n",
       "|Check|Check|\n",
       "|Answer|Answer|\n",
       "|Summarise|Summarise|\n",
       "|Quit|Quit|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table=\"|Query text|`OpTask`|\\n|--|--|\\n\"\n",
    "for s in Specials:\n",
    "    table+=\"|%s|%s|\\n\"%(s, [\"Check\", \"Answer\", \"Summarise\", \"Quit\"][Specials.index(s)])\n",
    "md(table)\n",
    "\n",
    "\n",
    "# If the query text is similar to one of the special texts,\n",
    "# then switch to the corresponding mode\n",
    "# Returns TRUE if mode is switched; FALSE otherwise\n",
    "def setmode(query_embedding):\n",
    "    for (i,s) in enumerate(Specials):\n",
    "        if cosine_similarity(query_embedding, SpecialEmbeds[i])>0.9:\n",
    "            # print(\"setmode: i=%d, s=%s\"%(i,s))\n",
    "            SpecialFunctions[i]()\n",
    "            setoptask()\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select which files to attach ##\n",
    "The easiest thing to do is to attach the top N files that are the most relevant to the query. The top N files are determined by computing the cosine-similarity of their segments to the query.\n",
    "\n",
    "The file score will be equal to the score of the closest segment in that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def findtopN(N, query_embedding):\n",
    "    global TextSimilarity, topN\n",
    "    TextSimilarity=np.ones(len(embeddings))*(-1)\n",
    "    for i in range(len(embeddings)):\n",
    "        similarity = cosine_similarity(embeddings[i][:-2], query_embedding)\n",
    "        itext=int(embeddings[i][-2])\n",
    "        if similarity>TextSimilarity[itext]:\n",
    "            TextSimilarity[itext]=similarity\n",
    "    topN=(np.argsort(TextSimilarity)[::-1])[0:N]\n",
    "    topN=topN.astype(int)\n",
    "    topN=topN.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the top N matching URLs ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prtopN():\n",
    "    s=\"**Query** : \"+Query\n",
    "    s=s+\"\\n\\nThe following are the top %d most relevant texts:\\n\\n\"%N\n",
    "    s=s+\"|itext|similarity|title|\\n|--|--|--|\\n\"\n",
    "    for i in topN:\n",
    "        s=s+\"|%d|%f|%s|\\n\"%(i,TextSimilarity[i],metadata[\"texts\"][i][1])\n",
    "    s=s+\"\\n\\nWe will attach these files to the thread.\\n\\n\"\n",
    "    md(s)\n",
    "    log(s)\n",
    "\n",
    "def showcheckpages(query):\n",
    "    s=\"**TOPIC** : \"+query+\"\\n\\n\"\n",
    "    for i in range(0,3):\n",
    "        s=s+\"**PAGE** : %s\\n\\n\"%(metadata[\"texts\"][topN[i]][1])\n",
    "        s=s+\"**URL** : %s\\n\\n\"%(metadata[\"texts\"][topN[i]][0])\n",
    "    md(s)\n",
    "    log(s)\n",
    "\n",
    "def showanswer(query):\n",
    "    msg=Client.beta.threads.messages.list(thread_id=Thread.id)\n",
    "    s=\"\\n\\n**Query** : \"+query+\"\\n\\n\"\n",
    "    s=s+\"**ANSWER** : \"+msg.data[0].content[0].text.value\n",
    "    md(s)\n",
    "    log(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Thread ##\n",
    "Create a thread with the query and the top N relevant files as information. In `qa.ipynb`, I attached files to the thread and I will continue this way even though in `playground.ipynb`, the files were attached to the Assistant.\n",
    "\n",
    "If thread attachment does not work, then I will attach files to the Assistant.  This is done by the [Modify Assistant](https://platform.openai.com/docs/api-reference/assistants/modifyAssistant) call. A list of File IDs can be attached this way. It sounds like this new file list overwrites the previous list. The API notes say that\n",
    ">If a file was previously attached to the list but does not show up in the list, it will be deleted from the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threadfiles():\n",
    "    return [UploadID[i] for i in topN]\n",
    "\n",
    "# First question in this topic\n",
    "def startthread(query):\n",
    "    global Thread, RunIDs\n",
    "    Thread = Client.beta.threads.create(messages=[\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "            \"file_ids\": threadfiles()\n",
    "          }\n",
    "        ]) \n",
    "    RunIDs=[] # When the thread is reset, the Run #ID list is also reset\n",
    "    return\n",
    "  \n",
    "# Subsequent questions in the same topic\n",
    "def qtothread(query):\n",
    "    global Thread\n",
    "\n",
    "    thread_message = Client.beta.threads.messages.create(\n",
    "      Thread.id,\n",
    "      role=\"user\",\n",
    "      content=query,\n",
    "      file_ids=threadfiles()\n",
    "    )\n",
    "     \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Query ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkfiles(run):\n",
    "    # Check the files attached to the run against the files in the thread\n",
    "    # return True if the files are the same; False otherwise\n",
    "    # print(\"Checking files\")\n",
    "    nrunfiles=len(run.file_ids)\n",
    "    threadfiles= [UploadID[i] for i in topN]\n",
    "    nthreadfiles=len(threadfiles)\n",
    "    if nrunfiles!=nthreadfiles:\n",
    "        return False\n",
    "    else:\n",
    "        for i in range(nrunfiles):\n",
    "            if run.file_ids[i]!=threadfiles[i]:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "def runquery():\n",
    "    extrainstructionaltext=\"You have access to the following files:\\n\\n\"+\\\n",
    "            \"\\nfile_ids: %s\"%(threadfiles())+ \"\\n\\nUse the files to answer the question.\"\n",
    "    run = Client.beta.threads.runs.create(\n",
    "            thread_id=Thread.id,\n",
    "            assistant_id=AssistantID,\n",
    "            instructions=Assistant.instructions)\n",
    "\n",
    "    print(\"THREAD # : %s, RUN # : %s\\nInstructions : %s\"%(Thread.id, run.id, run.instructions), end=\"\")\n",
    "    RunIDs.append(run.id)\n",
    "\n",
    "    print(run.status, end=\" \")\n",
    "    while run.status != \"completed\":\n",
    "        run = Client.beta.threads.runs.retrieve(thread_id=run.thread_id, run_id=run.id)\n",
    "        print(run.status[0], end=\"\")\n",
    "        if run.status == \"completed\":\n",
    "            print(\" \\u2713\", checkfiles(run))\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "    return run\n",
    "\n",
    "\n",
    "    \n",
    "# print(messages.data[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function lists the responses for all the runs on the current thread\n",
    "def show_all_messages():\n",
    "    s=\"\"\n",
    "    for i, run_id in enumerate(RunIDs):\n",
    "        steps_page=Client.beta.threads.runs.steps.list(run_id=run_id, thread_id=Thread.id)\n",
    "        message_id=steps_page.data[0].step_details.message_creation.message_id\n",
    "        message=Client.beta.threads.messages.retrieve(thread_id=Thread.id, message_id=message_id)\n",
    "        s=s+\"%02d - %s - %s\\n\\n%s\\n\\n\"%(i,Thread.id, run_id, message.content[0].text.value)\n",
    "        annotations=message.content[0].text.annotations\n",
    "        s+=\"\\n\\n%s\\n\\n\"%annotations\n",
    "    md(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quit\n"
     ]
    }
   ],
   "source": [
    "def chatloop():\n",
    "    global Query, Run, Thread\n",
    "    while(OpTask!=\"quit\"):\n",
    "        s=input(\"Enter command: \")\n",
    "        sembed=get_embedding(s)\n",
    "    # Check if the Query is similar to one of the strings in Specials:\n",
    "        if setmode(sembed):\n",
    "            continue # Continue to get the next command\n",
    "        if s!=\"\": # If empty string, use the last Query; otherwise update the Query\n",
    "            Query=s\n",
    "            query_embedding=get_embedding(Query)\n",
    "        if OpTask==\"check\":\n",
    "            findtopN(N, query_embedding)\n",
    "            prtopN()\n",
    "            showcheckpages(Query)\n",
    "            continue\n",
    "        if OpTask==\"answer\":\n",
    "            if Thread==None:\n",
    "                findtopN(N, query_embedding)\n",
    "                prtopN()\n",
    "                startthread(Query)\n",
    "            else:\n",
    "                qtothread(Query)\n",
    "            Run=runquery()\n",
    "            showanswer(Query)\n",
    "            \n",
    "chatloop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m thread_messages \u001b[38;5;241m=\u001b[39m Client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mlist(\u001b[43mThread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m)\n\u001b[1;32m      2\u001b[0m message_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(thread_messages)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(thread_messages.data)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# pr_objfields(thread_messages)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "\n",
    "thread_messages = Client.beta.threads.messages.list(Thread.id)\n",
    "message_list=list(thread_messages)\n",
    "# print(thread_messages.data)\n",
    "# pr_objfields(thread_messages)\n",
    "\n",
    "def tabulate_message(i):\n",
    "    message=message_list[i]\n",
    "    s=\"\\n\\n|Attribute|Value|\\n|---|---|\\n\"\n",
    "    s+=\"|ID|`%s`|\\n\"%message.id\n",
    "    s+=\"|Created|`%s`|\\n\"%(ts2lt(message.created_at))\n",
    "    s+=\"|Object name|`%s`|\\n\"%message.object\n",
    "    s+=\"|assistant_id|`%s`|\\n\"%message.assistant_id\n",
    "    s+=\"|thread_id|`%s`|\\n\"%message.thread_id\n",
    "    s+=\"|run_id|`%s`|\\n\"%message.run_id\n",
    "    s+=\"|file_ids|`%s`|\\n\"%message.file_ids\n",
    "    s+=\"|role|`%s`|\\n\"%message.role\n",
    "    s+=\"|content|`%s`|\\n\"%message.content\n",
    "    mdc(s)\n",
    "    mdc(\"\\n\\n\")\n",
    "\n",
    "mdc(\"\\n\\nThe following shows the fields of the six messages belonging to this thread. The messages are \\\n",
    "ordered in reverse. The last message \\\n",
    "is the initial question.  The second last message is the OpenAI answer to this question.\")\n",
    "\n",
    "for i in range(len(message_list)):\n",
    "    mdc(\"\\n\\n#### Message %d ####\\n\\n\"%i)\n",
    "    tabulate_message(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_Ql8V88Q4i0LXpXRngacVAxWg', created_at=1706683319, metadata={'modified': 'true', 'user': 'Q2'}, object='thread')\n"
     ]
    }
   ],
   "source": [
    "updated_thread = Client.beta.threads.update(\n",
    "  Thread.id,\n",
    "  extra_query={\"query\": \"Who did that study?\"},\n",
    "\n",
    "  metadata={\n",
    "    \"modified\": \"true\",\n",
    "    \"user\": \"Q2\"\n",
    "  }\n",
    ")\n",
    "print(updated_thread)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
