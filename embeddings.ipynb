{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CS1 - Install Libraries, define main variables, and some basic functions ####\n",
    "You can convert this notebook to HTML by entering the following command in the VS Code terminal window:\n",
    "\n",
    "`jupyter nbconvert --no-input --to html introduction.ipynb`\n",
    "\n",
    "_Make sure that the terminal window is running in the same virtual environment.  On my Mac computer, if the terminal prompt is `(.venv) $ ` then I know it is running in the virtual environment.  Otherwise, I enter `source venv/bin/activate` to put it into the virtual environment_\n",
    "\n",
    "**CS1, CS2, CS3, etc. are Code cells in the notebook.  The code does not appear in the HTML output.  You have to look into the notebook file (`introduction.ipynb`) to see the code contained in CS1, CS2, etc..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT :\n",
    "# I am running this in the local virtual environment venv.  The python libraries installed when running introduction.ipynb\n",
    "# do not have to be re-installed here as they are already installed in the virtual environment.\n",
    "#\n",
    "# The following are the additional libraries used in the notebook\n",
    "#\n",
    "# I set my OPENAI_API_KEY in a .env file.  You can also set it in your environment variables.\n",
    "# The following two lines read the .env file and set the environment variable.\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import melib\n",
    "from melib.xt import mdx\n",
    "\n",
    "# The following are the variables used in the notebook\n",
    "\n",
    "PIE=math.pi\n",
    "SECTION=0\n",
    "Chapter=\"embeddings\"\n",
    "#\n",
    "# Define the md() function to display markdown text\n",
    "from IPython.display import display, Markdown\n",
    "def md(s):\n",
    "    display(Markdown(s))\n",
    "\n",
    "# Establish OpenAI API key (see below for how to get one)\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI()\n",
    "LLM=\"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CS1 Ends ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS2 ####\n",
    "Program flow control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_EMBEDDINGS=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS2 Ends ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CS3 ####\n",
    "Embedding computations.  The results are used in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function calculates the factors of the integer N\n",
    "# I use it to find X and Y factors of the embedding vector size N\n",
    "# For N=1536, which is the length of the embeddings created by OpenAI A, use X=32 and Y=48\n",
    "def calculate_factors(N):\n",
    "    factors = []\n",
    "    for i in range(1, N+1):\n",
    "        if N % i == 0:\n",
    "            factors.append(i)\n",
    "    return factors\n",
    "# N = 1536\n",
    "# factors = calculate_factors(N)\n",
    "# print(factors)\n",
    "# print(1536/48)\n",
    "#\n",
    "# The following function maps a vector to a matrix of size X by Y\n",
    "def map_vector_to_array(vector, X, Y):\n",
    "    A = np.reshape(vector, (X, Y))\n",
    "    return A\n",
    "# print(map_vector_to_array([1,2,3,4,5,6,7,8,9,10,11,12], 3, 4))\n",
    "#\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "def pickcolor(a, s):\n",
    "    if s=='NP':\n",
    "        if a < 0:\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'black'\n",
    "    return color\n",
    "#\n",
    "# The following function visualizes the array\n",
    "# The coloring choices are 'NP' for negative-positive or 'viridis' for the viridis colormap\n",
    "\n",
    "def visualize_array(A, coloring=\"NP\"):\n",
    "    # Create a new figure\n",
    "    plt.figure()\n",
    "\n",
    "    # Get the dimensions of the array\n",
    "    rows, cols = A.shape\n",
    "\n",
    "    if coloring == \"NP\":\n",
    "    # Iterate over each element in the array\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                marker='s'\n",
    "                # Pick the color\n",
    "                color = pickcolor(A[i, j], coloring)\n",
    "\n",
    "                # Plot the element on the x-y plot\n",
    "                plt.plot(j, i, marker, color=color)\n",
    "    elif coloring == \"viridis\":\n",
    "        plt.imshow(A, cmap='viridis')\n",
    "        plt.colorbar()\n",
    "\n",
    "    # Set the x and y axis labels\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a text file into a string\n",
    "def read_text_file(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "    return data\n",
    "\n",
    "# Generate the embedding for the text\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=LLM\n",
    "    )\n",
    "    return np.array(response.data[0].embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function calculates the dot product of two vectors\n",
    "def dot_product(v1, v2):\n",
    "    return np.dot(v1, v2)\n",
    "\n",
    "# The following function calculates the cosine similarity of two vectors\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# The following function searches for the closest vector in the embedding space.\n",
    "# It returns the index of the closest vector and the cosine similarity\n",
    "def find_closest_vector(v, vectors):\n",
    "    similarity = -1\n",
    "    index = -1\n",
    "    for i in range(len(vectors)):\n",
    "        s = cosine_similarity(v, vectors[i])\n",
    "        if s > similarity:\n",
    "            similarity = s\n",
    "            index = i\n",
    "    return index, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your string here.  Then run the following cell to get the embedding\n",
    "sa=[\"Hello World!\",\n",
    "\"One, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve\",\n",
    "read_text_file(\"data/ozconstitution.txt\"),\n",
    "read_text_file(\"data/elon.txt\"),\n",
    "read_text_file(\"data/buildings.txt\"),\n",
    "read_text_file(\"data/pascal.txt\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings from file\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_EMBEDDINGS:\n",
    "    embeddings=[]\n",
    "    for s in sa:\n",
    "        embedding=get_embedding(s)\n",
    "        embeddings.append(embedding)\n",
    "    np.save(\"data/embeddings.npy\", embeddings)\n",
    "else:\n",
    "    embeddings=np.load(\"data/embeddings.npy\", allow_pickle=True)\n",
    "    print(\"Loaded embeddings from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806\n"
     ]
    }
   ],
   "source": [
    "iembed=1\n",
    "v=embeddings[iembed]\n",
    "A=map_vector_to_array(v, 32,48)\n",
    "# Count the number of negative numbers in the array\n",
    "print(np.sum(A < 0))\n",
    "# Visualize the array\n",
    "visualize_array(A,\"viridis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The query = What is the first name of the person who bought Twitter?\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The closest source is 3 ('Mustafa Kemal Atat√ºrk died 85 ') with a similarity of 77.68%\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s1=\"Do lorikeets like being kept in cages?\"\n",
    "s2=\"When did Turkey have an earthquake?\"\n",
    "s3=\"Kindness is a friendly hello\"\n",
    "s4=\"What is the first name of the person who bought Twitter?\"\n",
    "s=s4\n",
    "v=get_embedding(s)\n",
    "if v is None:\n",
    "    print(\"No embedding for string \", s)\n",
    "else:\n",
    "    (i,similarity)=find_closest_vector(v, embeddings)\n",
    "    md(\"The query = %s\\n\\n\"%s)\n",
    "    md(\"The closest source is %d ('%s') with a similarity of %.2f%%\\n\\n\"%(i, sa[i][:30], similarity*100))\n",
    "    # md(\"Closest vector is \"+sa[i]+\" with similarity \"+\"%f\"%similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CS3 Ends ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# EMBEDDINGS #\n",
       "\n",
       "#### Table of Contents ####\n",
       "\n",
       "_2023_\n",
       "\n",
       "|Section|Title|\n",
       "|:------|:-------|\n",
       "|1|<a href=\"#What-is-an-embedding?\">What is an embedding?</a>|\n",
       "|2|<a href=\"#How-to-measure-distance-between-two-embeddings?\">How to measure distance between two embeddings?</a>|\n",
       "|3|<a href=\"#The-optimal-size-for-the-text-sections\">The optimal size for the text sections</a>|\n",
       "\n",
       "\n",
       "This notebook is about embeddings. I will use OpenAI API to generate embeddings. To run this notebook you need to have an OpenAI account.  Do not try to run this notebook without reading my first notebook, [`introduction,ipynb`](https://github.com/Gurgenci/probot/blob/main/introduction.ipynb) in this series.The link to that notebook can be found on [27 November 2023 my blog post](https://halimgur.substack.com/p/training-chatbots-to-become-professionals).  On that [post](https://halimgur.substack.com/p/training-chatbots-to-become-professionals) and in the [notebook](https://github.com/Gurgenci/probot/blob/main/introduction.ipynb) `introduction.ipynb`, I show how to get an OpenAI account as well as a few other things.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TOC=[\"What is an embedding?\", \"How to measure distance between two embeddings?\",  \\\n",
    "     \"The optimal size for the text sections\"\n",
    "     ]\n",
    "MD=mdx(Chapter, SECTION, title=\"EMBEDDINGS\")\n",
    "MD.toc(TOC,\"2023\")\n",
    "#\n",
    "# \n",
    "MD.write('This notebook is about embe\\\n",
    "ddings. I will use OpenAI API to generate embeddin\\\n",
    "gs. To run this notebook you need to have an OpenA\\\n",
    "I account.  Do not try to run this notebook without reading my first notebook, \\\n",
    "[`introduction,ipynb`](https://github.com/Gurgenci/probot/blob/main/introduction.ipynb) in this series.\\\n",
    "The link to that notebook can be found on [27 November 2023 my blog post]\\\n",
    "(https://halimgur.substack.com/p/training-chatbots-to-become-professionals).  \\\n",
    "On that [post](https://halimgur.substack.com/p/training-chatbots-to-become-professionals) \\\n",
    "and in the [notebook](https://github.com/Gurgenci/probot/blob/main/introduction.ipynb) `introduction.ipynb`, I show how to get an OpenAI account \\\n",
    "as well as a few other things.\\n\\n')\n",
    "if RECREATE_EMBEDDINGS:\n",
    "     MD.write(\"**Important** : EMbeddings were recreated and saved into the `data` folder. \\\n",
    "              Make RECREATE_EMBEDDINGS False if you do not want to recreate the embeddings in future RUN ALLs.  \\n\\n\")\n",
    "md(MD.out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# What is an embedding? #\n",
       "\n",
       "An embedding is a vector of numbers that represents a text.           For example, using the method explained further below, I generated the following embedding vector for the text string `Hello World!`:\n",
       "\n",
       "         \n",
       "\n",
       "|Dimension|Value|\n",
       "|--|--|\n",
       "|1|-0.002|\n",
       "|2|-0.026|\n",
       "|3|-0.009|\n",
       "|...|...|\n",
       "|1534|-0.010|\n",
       "|1535|0.008|\n",
       "|1536|-0.005|\n",
       "\n",
       "\n",
       "The following image shows how I generate the embedding vector by running my python function `get_embedding()`         which you can find in CS3 at the beginning of this          notebook.  The _roller_ in the image is that function.  I feed the text string to the OpenAI API and it returns the embedding vector,          which is falling down off the roller like a spaghetti string.\n",
       "\n",
       "![alt text](pics/halembeds.jpg 'halembeds.jpg')\n",
       "\n",
       "<i>Figure 1.1. </i>\n",
       "\n",
       "\n",
       "\n",
       "### Calling OpenAI to generate the embedding vector ###\n",
       "\n",
       "I use the `get_embedding()` function to generate the embedding vector.  The following lines in that function gets          the OpenAI API compute the embedding and the function returns it as a `numpy` array (`np` stands for `numpy`):\n",
       "\n",
       "```python\n",
       "    response = client.embeddings.create(\n",
       "        input=text,\n",
       "        model=\"text-embedding-ada-002\"\n",
       "    )\n",
       "    return np.array(response.data[0].embedding)\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "### The length of the embedding vector ###\n",
       "\n",
       "The length of an embedding vector is fixed for a given model.  For example, the model `text-embedding-ada-002`          generates an embedding vector of length 1536.  Whether the input text is only of two words (e.g. 'Hello World!' or whether it is the          entire text of one blog post (e.g. https://halimgur.substack.com/p/why-did-elon-musk-buy-twitter),          its embedding vector will be 1536 long when using text-embedding-ada-002 \n",
       "\n",
       "         \n",
       "\n",
       "### Visualisation of the embedding vector ###\n",
       "\n",
       "Visualizing the embedding vector for large language models (LLMs) like 'text-embedding-ada-002' presents a challenge due to the high dimensionality of the vector space (typically 1536 dimensions for text-embedding-ada-002). People have used     dimensionality reduction techniques like PCA and t-SNE to reduce the dimensionality.  I am     not going to use those techniques in this notebook.  I am interested in simple things like how the number of          negative and positive entries and magnitudes change.  This is not useful knowledge but I was curious.          I reshaped the embedding vector to a 32 by 48 matrix and plotted it by coloring the negatives red          and positives black. The following are the visualisation of the first three embeddings:\n",
       "\n",
       "![alt text](pics/embeddings_np.jpg 'embeddings_np.jpg')\n",
       "\n",
       "<i>Figure 1.2. </i>\n",
       "\n",
       "\n",
       "\n",
       "The first two are short strings and the last one is the preamble of the Australian Constitution.           The number of positive and negative entries are in the table below:\n",
       "\n",
       "\n",
       "\n",
       "|String|Positive|Negative|\n",
       "|--|--|--|\n",
       "|Hello World!|763|773|\n",
       "|One, two, three, four, five, s|730|806|\n",
       "|The Australian Constitution ha|777|759|\n",
       "\n",
       "\n",
       "As you can see, there is not a significant difference between the number of positive and negative entries.           I then plotted the embeddings using the `viridis` colormap, which is a utility offered by          the `matplotlib` library. CS2 section above in this notebook has the function. The following are the results:\n",
       "\n",
       "![alt text](pics/embeddings_viridis.jpg 'embeddings_viridis.jpg')\n",
       "\n",
       "<i>Figure 1.3. </i>\n",
       "\n",
       "\n",
       "\n",
       "         \n",
       "\n",
       "The color plots suggest that all three embeddings have their minimum member at the top left corner.           The following table shows the index and the value of the minimum and maximum values for all six embeddings:\n",
       "\n",
       "\n",
       "\n",
       "|String|String Length|Min Index|Min Value|Max Index|Max Value|\n",
       "|--|--|--|--|--|--|\n",
       "|Hello World!|12|194|-0.691|954|0.231|\n",
       "|One, two, three, four, five, s|73|194|-0.692|954|0.222|\n",
       "|The Australian Constitution ha|2387|194|-0.639|954|0.179|\n",
       "|Mustafa Kemal Atat√ºrk died 85 |13428|194|-0.651|954|0.221|\n",
       "|All those workshops, changed l|17325|194|-0.649|954|0.208|\n",
       "|Pascal, whom I mentioned on my|7522|194|-0.640|954|0.221|\n",
       "\n",
       "\n",
       "This is interesting but not useful.  I am going to move on to more useful things.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SECTION+=1\n",
    "SECTION=1\n",
    "MD=mdx(Chapter, SECTION, TOC[SECTION-1])\n",
    "MD.write(\"An embedding is a vector of numbers that represents a text.  \\\n",
    "         For example, using the method explained further below, I generated the following embedding vector for the text string `%s`:\\n\\n\\\n",
    "         \"%sa[0])\n",
    "MD.write(\"\\n\\n|Dimension|Value|\\n|--|--|\\n\")\n",
    "combined_list = [i for i in range(0, 3)] + [i for i in range(1533, 1536)]\n",
    "for i in combined_list:\n",
    "    MD.write(\"|%d|%.3f|\\n\"%(i+1,v[i]))\n",
    "    if i==2:\n",
    "        MD.write(\"|...|...|\\n\")\n",
    "MD.write(\"\\n\\n\")\n",
    "MD.write(\"The following image shows how I generate the embedding vector by running my python function `get_embedding()`\\\n",
    "         which you can find in CS3 at the beginning of this \\\n",
    "         notebook.  The _roller_ in the image is that function.  I feed the text string to the OpenAI API and it returns the embedding vector, \\\n",
    "         which is falling down off the roller like a spaghetti string.\\n\\n\\\n",
    ":::3|halembeds.jpg::\\n\\n\")\n",
    "MD.write('### Calling OpenAI to generate the embedding vector ###\\n\\n\\\n",
    "I use the `get_embedding()` function to generate the embedding vector.  The following lines in that function gets \\\n",
    "         the OpenAI API compute the embedding and the function returns it as a `numpy` array (`np` stands for `numpy`):\\n\\n\\\n",
    "```python\\n\\\n",
    "    response = client.embeddings.create(\\n\\\n",
    "        input=text,\\n\\\n",
    "        model=\"text-embedding-ada-002\"\\n\\\n",
    "    )\\n\\\n",
    "    return np.array(response.data[0].embedding)\\n\\\n",
    "```\\n\\n')\n",
    "MD.write(\"\\n\\n### The length of the embedding vector ###\\n\\n\\\n",
    "The length of an embedding vector is fixed for a given model.  For example, the model `text-embedding-ada-002` \\\n",
    "         generates an embedding vector of length 1536.  Whether the input text is only of two words (e.g. 'Hello World!' or whether it is the \\\n",
    "         entire text of one blog post (e.g. https://halimgur.substack.com/p/why-did-elon-musk-buy-twitter), \\\n",
    "         its embedding vector will be %d long when using %s \\n\\n\\\n",
    "         \"%(len(v), LLM))\n",
    "MD.write(\"\\n\\n### Visualisation of the embedding vector ###\\n\\n\")\n",
    "MD.write(\"Visualizing the embedding vector for large language models (LLMs) like\\\n",
    " 'text-embedding-ada-002' presents a challenge due to the high dimensionality of\\\n",
    " the vector space (typically 1536 dimensions for text-embedding-ada-002). People have used \\\n",
    "    dimensionality reduction techniques like PCA and t-SNE to reduce the dimensionality.  I am \\\n",
    "    not going to use those techniques in this notebook.  I am interested in simple things like how the number of \\\n",
    "         negative and positive entries and magnitudes change.  This is not useful knowledge but I was curious. \\\n",
    "         I reshaped the embedding vector to a 32 by 48 matrix and plotted it by coloring the negatives red \\\n",
    "         and positives black. The following are the visualisation of the first three embeddings:\\n\\n\\\n",
    ":::3|embeddings_np.jpg::\\n\\n\\\n",
    "The first two are short strings and the last one is the preamble of the Australian Constitution.  \\\n",
    "         The number of positive and negative entries are in the table below:\\n\\n\")\n",
    "MD.write(\"\\n\\n|String|Positive|Negative|\\n|--|--|--|\\n\")\n",
    "for i in range(3):\n",
    "    v=embeddings[i]\n",
    "    A=map_vector_to_array(v, 32,48)\n",
    "    MD.write(\"|%s|%d|%d|\\n\"%(sa[i][:30], np.sum(A > 0), np.sum(A < 0)))\n",
    "MD.write(\"\\n\\n\")\n",
    "MD.write(\"As you can see, there is not a significant difference between the number of positive and negative entries.  \\\n",
    "         I then plotted the embeddings using the `viridis` colormap, which is a utility offered by \\\n",
    "         the `matplotlib` library. CS2 section above in this notebook has the function. The following are the results:\\n\\n\\\n",
    ":::3|embeddings_viridis.jpg::\\n\\n\\\n",
    "         \")\n",
    "MD.write(\"\\n\\n\")\n",
    "MD.write(\"The color plots suggest that all three embeddings have their minimum member at the top left corner.  \\\n",
    "         The following table shows the index and the value of the minimum and maximum values for all six embeddings:\\n\\n\")\n",
    "MD.write(\"\\n\\n|String|String Length|Min Index|Min Value|Max Index|Max Value|\\n|--|--|--|--|--|--|\\n\")\n",
    "for i in range(6):\n",
    "    v=embeddings[i]\n",
    "    min_value = np.min(v)\n",
    "    min_index = np.argmin(v)\n",
    "    max_value = np.max(v)\n",
    "    max_index = np.argmax(v)\n",
    "    MD.write(\"|%s|%d|%d|%.3f|%d|%.3f|\\n\"%(sa[i][:30], len(sa[i]), min_index, min_value, max_index, max_value))\n",
    "    # A=map_vector_to_array(v, 32,48)\n",
    "    # min_index = np.unravel_index(np.argmin(A, axis=None), A.shape)\n",
    "    # max_index = np.unravel_index(np.argmax(A, axis=None), A.shape)\n",
    "    # MD.write(\"|%s|%d|%.3f|%d|%.3f|\\n\"%(sa[i][:30], min_index[0]*48+min_index[1], A[min_index], max_index[0]*48+max_index[1], A[max_index]))\n",
    "MD.write(\"\\n\\n\")\n",
    "MD.write(\"This is interesting but not useful.  I am going to move on to more useful things.\\n\\n\")\n",
    "md(MD.out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# How to measure distance between two embeddings? #\n",
       "\n",
       "The cosine similarity is a measure of similarity between two vectors.  The cosine similarity of two vectors          is the cosine of the angle between them.  It is calculated as follows:\n",
       "\n",
       "$$cosine\\_similarity = \\frac{A \\cdot B}{||A|| \\cdot ||B||}$$\n",
       "\n",
       "where $A$ and $B$ are the two vectors and $||A||$ is the norm of $A$.\n",
       "\n",
       "Those of you who know their linear algebra will know that the dot product of two vectors is the product of their          magnitudes and the cosine of the angle between them.  The cosine similarity is the dot product of the two vectors          divided by the product of their magnitudes.  The cosine similarity is a number between -1 and 1.  If the cosine          similarity is 1, then the two vectors are identical.  If the cosine similarity is -1, then the two vectors are          opposite to each other.  If the cosine similarity is 0, then the two vectors are orthogonal to each other.\n",
       "\n",
       "The following are the cosine similarities of the first three embeddings with themselves:\n",
       "\n",
       "\n",
       "\n",
       "|String|String Length|Cosine Similarity|\n",
       "|--|--|--|\n",
       "|Hello World!|12|1.000|\n",
       "|One, two, three, four, five, s|73|1.000|\n",
       "|The Australian Constitution ha|2387|1.000|\n",
       "|Mustafa Kemal Atat√ºrk died 85 |13428|1.000|\n",
       "|All those workshops, changed l|17325|1.000|\n",
       "|Pascal, whom I mentioned on my|7522|1.000|\n",
       "\n",
       "\n",
       "As you can see, the cosine similarity of the embedding of a string with itself is 1.           The following are the cosine similarities of the first three embeddings with the embedding of the          preamble of the Australian Constitution:\n",
       "\n",
       "\n",
       "\n",
       "|String|String Length|Cosine Similarity|\n",
       "|--|--|--|\n",
       "|Hello World!|12|0.709|\n",
       "|One, two, three, four, five, s|73|0.706|\n",
       "|The Australian Constitution ha|2387|1.000|\n",
       "|Mustafa Kemal Atat√ºrk died 85 |13428|0.730|\n",
       "|All those workshops, changed l|17325|0.744|\n",
       "|Pascal, whom I mentioned on my|7522|0.693|\n",
       "\n",
       "\n",
       "Let us compute the cosine similarity of the query `When did Mustafa Kemal Ataturk die?` with our six embeddings:\n",
       "\n",
       "\n",
       "\n",
       "|String|String Length|Cosine Similarity|\n",
       "|--|--|--|\n",
       "|Hello World!|12|0.709|\n",
       "|One, two, three, four, five, s|73|0.717|\n",
       "|The Australian Constitution ha|2387|0.692|\n",
       "|Mustafa Kemal Atat√ºrk died 85 |13428|0.863|\n",
       "|All those workshops, changed l|17325|0.769|\n",
       "|Pascal, whom I mentioned on my|7522|0.739|\n",
       "\n",
       "\n",
       "As you can see it is favouring the fourth string, which is my blog post on Elon Musk.  I started that post          with a brief note on November 10th, which is the date Mustafa Kemal Ataturk died. But it is only a small part of          the post.  This is probably why the cosine similarity is not very high.\n",
       "\n",
       "If you download the notebook, then you can run this cell with different          queries to see how the cosine similarity changes.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SECTION+=1\n",
    "SECTION=2\n",
    "MD=mdx(Chapter, SECTION, TOC[SECTION-1])\n",
    "MD.write(\"The cosine similarity is a measure of similarity between two vectors.  The cosine similarity of two vectors \\\n",
    "         is the cosine of the angle between them.  It is calculated as follows:\\n\\n\\\n",
    "$$cosine\\_similarity = \\\\frac{A \\\\cdot B}{||A|| \\\\cdot ||B||}$$\\n\\n\\\n",
    "where $A$ and $B$ are the two vectors and $||A||$ is the norm of $A$.\\n\\n\")\n",
    "MD.write(\"Those of you who know their linear algebra will know that the dot product of two vectors is the product of their \\\n",
    "         magnitudes and the cosine of the angle between them.  The cosine similarity is the dot product of the two vectors \\\n",
    "         divided by the product of their magnitudes.  The cosine similarity is a number between -1 and 1.  If the cosine \\\n",
    "         similarity is 1, then the two vectors are identical.  If the cosine similarity is -1, then the two vectors are \\\n",
    "         opposite to each other.  If the cosine similarity is 0, then the two vectors are orthogonal to each other.\\n\\n\")\n",
    "MD.write(\"The following are the cosine similarities of the first three embeddings with themselves:\\n\\n\")\n",
    "MD.write(\"\\n\\n|String|String Length|Cosine Similarity|\\n|--|--|--|\\n\")\n",
    "for i in range(6):\n",
    "    v=embeddings[i]\n",
    "    similarity = cosine_similarity(v, v)\n",
    "    MD.write(\"|%s|%d|%.3f|\\n\"%(sa[i][:30], len(sa[i]), similarity))\n",
    "MD.write(\"\\n\\n\")\n",
    "MD.write(\"As you can see, the cosine similarity of the embedding of a string with itself is 1.  \\\n",
    "         The following are the cosine similarities of the first three embeddings with the embedding of the \\\n",
    "         preamble of the Australian Constitution:\\n\\n\")\n",
    "MD.write(\"\\n\\n|String|String Length|Cosine Similarity|\\n|--|--|--|\\n\")\n",
    "for i in range(6):\n",
    "    v=embeddings[i]\n",
    "    similarity = cosine_similarity(v, embeddings[2])\n",
    "    MD.write(\"|%s|%d|%.3f|\\n\"%(sa[i][:30], len(sa[i]), similarity))\n",
    "query=\"When did Mustafa Kemal Ataturk die?\"\n",
    "THIS=False # Make this false if you do not want to keep creating the query embedding\n",
    "if THIS:\n",
    "    query_embedding=get_embedding(query)\n",
    "MD.write(\"\\n\\nLet us compute the cosine similarity of the query `%s` with our six embeddings:\\n\\n\"%query)\n",
    "MD.write(\"\\n\\n|String|String Length|Cosine Similarity|\\n|--|--|--|\\n\")\n",
    "for i in range(6):\n",
    "    v=embeddings[i]\n",
    "    similarity = cosine_similarity(v, query_embedding)\n",
    "    MD.write(\"|%s|%d|%.3f|\\n\"%(sa[i][:30], len(sa[i]), similarity))\n",
    "MD.write(\"\\n\\nAs you can see it is favouring the fourth string, which is my blog post on Elon Musk.  I started that post \\\n",
    "         with a brief note on November 10th, which is the date Mustafa Kemal Ataturk died. But it is only a small part of \\\n",
    "         the post.  This is probably why the cosine similarity is not very high.\\n\\n\")\n",
    "MD.write(\"If you download the notebook, then you can run this cell with different \\\n",
    "         queries to see how the cosine similarity changes.\\n\\n\")\n",
    "MD.write(\"\\n\\n\")\n",
    "md(MD.out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# The optimal size for the text sections #\n",
       "\n",
       "According to Bard **and** ChatGPT, there is not an easy rule to determine the optimal size of the text sections.            ChatGPT says that as a rough guideline, in many NLP applications, text sections ranging from a few sentences to a few          paragraphs (roughly 100 to 500 words) are commonly used, but this can vary significantly based on the specific          use case and requirements.\n",
       "\n",
       "Let us calculate the number of words in the first three strings:\n",
       "\n",
       "\n",
       "\n",
       "|String|String Length|Number of Words|\n",
       "|--|--|--|\n",
       "|Hello World!|12|2|\n",
       "|One, two, three, four, five, s|73|12|\n",
       "|The Australian Constitution ha|2387|352|\n",
       "|Mustafa Kemal Atat√ºrk died 85 |13428|2178|\n",
       "|All those workshops, changed l|17325|2762|\n",
       "|Pascal, whom I mentioned on my|7522|1350|\n",
       "\n",
       "\n",
       "It looks like some of my strings are too long.  It is easy to split them to smaller segments but          this may cause other problems.  For example, if I do the segmentation automatically, part of          an important sentence can be in one segment and the other part in another segment.          This may cause the loss of the information included in that segment.           It is possible to have them overlapping each other.  The best option of course is          to obey the optimum segment size condition while preparing the corpus.  But this requires          knowing what that optimal number is.  This needs further thinking and probably some experimentation.\n",
       "\n",
       "I asked Bard for some references in this area.  It gave me the following:\n",
       "\n",
       "* `RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks by Lewis et al. (2020)`:This paper introduces the RAG model and discusses the potential benefits of using different text section sizes for retrieval and generation.While the paper doesn't provide specific optimal sizes, it emphasizes the importance of experimenting with different sizes based on the task and dataset.Link: https://arxiv.org/abs/2005.11401\n",
       "* `Exploring the Impact of Text Chunk Size in Dense Retrievers by Chen et al. (2022)`:    This paper investigates the impact of text chunk size on the performance of dense retrieval models, which are often used as the retrieval component in RAG models.Their findings suggest that a moderate chunk size (e.g., 512 tokens) can achievea good balance between retrieval accuracy and computational efficiency.Link: https://arxiv.org/abs/2211.14876\n",
       "* `Adaptive Text Chunk Size for Efficient Dense Retrieval by Wang et al. (2022)`:This paper proposes an adaptive text chunking approach that dynamically adjusts the text chunk size based on the document length and complexity.Their results demonstrate that this approach can improve retrieval accuracy and efficiency compared to using a fixed chunk size.Link: https://arxiv.org/abs/2205.03284\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SECTION+=1\n",
    "SECTION=3\n",
    "MD=mdx(Chapter, SECTION, TOC[SECTION-1])\n",
    "MD.write(\"According to Bard **and** ChatGPT, there is not an easy rule to determine the optimal size of the text sections.  \\\n",
    "          ChatGPT says that as a rough guideline, in many NLP applications, text sections ranging from a few sentences to a few \\\n",
    "         paragraphs (roughly 100 to 500 words) are commonly used, but this can vary significantly based on the specific \\\n",
    "         use case and requirements.\\n\\n\\\n",
    "Let us calculate the number of words in the first three strings:\\n\\n\")\n",
    "MD.write(\"\\n\\n|String|String Length|Number of Words|\\n|--|--|--|\\n\")\n",
    "for i in range(6):\n",
    "    MD.write(\"|%s|%d|%d|\\n\"%(sa[i][:30], len(sa[i]), len(sa[i].split())))\n",
    "MD.write(\"\\n\\nIt looks like some of my strings are too long.  It is easy to split them to smaller segments but \\\n",
    "         this may cause other problems.  For example, if I do the segmentation automatically, part of \\\n",
    "         an important sentence can be in one segment and the other part in another segment. \\\n",
    "         This may cause the loss of the information included in that segment.  \\\n",
    "         It is possible to have them overlapping each other.  The best option of course is \\\n",
    "         to obey the optimum segment size condition while preparing the corpus.  But this requires \\\n",
    "         knowing what that optimal number is.  This needs further thinking and probably some experimentation.\\n\\n\")\n",
    "MD.write(\"I asked Bard for some references in this area.  It gave me the following:\\n\\n\")\n",
    "MD.write(\"* `RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks by Lewis et al. (202\\\n",
    "0)`:This paper introduces the RAG model and discusses the potential benefits of using different text \\\n",
    "section sizes for retrieval and generation.While the paper doesn't provide specific optimal sizes, i\\\n",
    "t emphasizes the importance of experimenting with different sizes based on the task and dataset.Link\\\n",
    ": https://arxiv.org/abs/2005.11401\\n\")\n",
    "MD.write(\"* `Exploring the Impact of Text Chunk Size in Dense Retrievers by Chen et al. (2022)`:    This\\\n",
    " paper investigates the impact of text chunk size on the performance of dense retrieval models, \\\n",
    "which are often used as the retrieval component in RAG models.Their findings suggest that a moderate\\\n",
    " chunk size (e.g., 512 tokens) can achievea good balance between retrieval accuracy and computationa\\\n",
    "l efficiency.Link: https://arxiv.org/abs/2211.14876\\n\")\n",
    "MD.write(\"* `Adaptive Text Chunk Size for Efficient Dense Retrieval by Wang et al. (2022)`:This paper \\\n",
    "proposes an adaptive text chunking approach that dynamically adjusts the text chunk size based on th\\\n",
    "e document length and complexity.Their results demonstrate that this approach can improve retrieval \\\n",
    "accuracy and efficiency compared to using a fixed chunk size.Link: https://arxiv.org/abs/2205.03284\"\\\n",
    ")\n",
    "MD.write(\"\\n\\n\")\n",
    "md(MD.out())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
