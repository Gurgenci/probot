{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CS1 - Install Libraries, define main variables, and some basic functions ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT :\n",
    "# Before you run this cell, make sure you create a virtual environment.  See below for how.\n",
    "# DO NOT INSTALL ANYTHING IN THE BASE ENVIRONMENT.\n",
    "#\n",
    "# The following are the libraries used in the notebook\n",
    "%pip install --q melib\n",
    "%pip install --q pylatex\n",
    "%pip install --q openpyxl\n",
    "%pip install --q setuptools\n",
    "%pip install --q --upgrade openai\n",
    "%pip install --q nbconvert\n",
    "import math\n",
    "import numpy as np\n",
    "import melib\n",
    "from melib.xt import mdx\n",
    "from melib.excel import Xcel\n",
    "from melib.library import steelprop\n",
    "# The following are the variables used in the notebook\n",
    "\n",
    "PIE=math.pi\n",
    "SECTION=0\n",
    "Chapter=\"Embeddings\"\n",
    "#\n",
    "TextString1=\"There are four sides to a square.\"\n",
    "TextString2=\"There are three sides to a triangle.\"\n",
    "TextString3=\"There are five sides to a pentagon.\"\n",
    "TextString4=\"In a footbal game, there are eleven players on each team.\"\n",
    "\n",
    "# The following are the functions used in the notebook\n",
    "from IPython.display import display, Markdown\n",
    "def md(s):\n",
    "    display(Markdown(s))\n",
    "\n",
    "# The following will print the version of the package:import pkg_resources\n",
    "def packageversion(package_name):\n",
    "    import pkg_resources\n",
    "    version = pkg_resources.get_distribution(package_name).version\n",
    "    print(f\"The version of {package_name} is: {version}\")\n",
    "\n",
    "# Establish OpenAI API key (see below for how to get one)\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CS1 ends ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CS2 - OpenAI calls ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# response = client.embeddings.create(\n",
    "#     input=\"Empty\",\n",
    "#     model=\"text-embedding-ada-002\"\n",
    "# )\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return np.array(response.data[0].embedding)\n",
    "\n",
    "def get_similarity(text1, text2):\n",
    "    response = client.embeddings.similarity(\n",
    "        texts=[text1, text2],\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].score\n",
    "\n",
    "def get_distance(text1, text2):\n",
    "    response = client.embeddings.distance(\n",
    "        texts=[text1, text2],\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].distance\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    response = client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return np.array(response.data[0].embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CS2 Ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CS3 - Embedding vector computations.  The results are referred to in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99999998 1.00000003 0.99999998 1.00000004]\n"
     ]
    }
   ],
   "source": [
    "Emvectors=np.array([get_embedding(TextString1),get_embedding(TextString2),get_embedding(TextString3),get_embedding(TextString4)])\n",
    "Magnitudes=np.linalg.norm(Emvectors,axis=1)\n",
    "print(Magnitudes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CS3 Ends ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# TEXT EMBEDDINGS #\n",
       "\n",
       "#### Table of Contents ####\n",
       "\n",
       "_2023_\n",
       "\n",
       "|Section|Title|\n",
       "|:------|:-------|\n",
       "|1|<a href=\"#Introduction\">Introduction</a>|\n",
       "|2|<a href=\"#Please-Join-Me\">Please Join Me</a>|\n",
       "|3|<a href=\"#Install-Python\">Install Python</a>|\n",
       "|4|<a href=\"#Set-up-Virtual-Environment\">Set up Virtual Environment</a>|\n",
       "|5|<a href=\"#Establish-OpenAI-Credentials\">Establish OpenAI Credentials</a>|\n",
       "|6|<a href=\"#What-is-an-embedding?\">What is an embedding?</a>|\n",
       "|7|<a href=\"#How-to-create-an-embedding-vector\">How to create an embedding vector</a>|\n",
       "\n",
       "\n",
       "Text Embeddings used in Large Language Models (LLMs)\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TOC=[\"Introduction\", \"Please Join Me\", \"Install Python\", \"Set up Virtual Environment\", \"Establish OpenAI Credentials\", \"What is an embedding?\", \"How to create an embedding vector\"]\n",
    "MD=mdx(Chapter, SECTION, title=\"TEXT EMBEDDINGS\")\n",
    "MD.toc(TOC,\"2023\")\n",
    "#\n",
    "# \n",
    "MD.write(\"Text Embeddings used in Large Language Models (LLMs)\\n\\n\")\n",
    "md(MD.out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Introduction #\n",
       "\n",
       "In March 2023, [I wrote in my blog](https://halimgur.substack.com/p/competent-intelligence-is-here-will) that the SOA LLMs were like high-school graduates:\n",
       "\n",
       "* Knows how to read and write\n",
       "* Thinks they know everything\n",
       "\n",
       "When you ask them a question you always get an answer because, if they do not know the answer, they would make it up\n",
       "\n",
       "In March 2023, I said that LLMs were not yet competent enough to deliver professional functions such as engineering. A competent intelligence would be equivalent to a college graduate.  GPT-4 or even a future GPT-5 would have to be further trained to get there.  Today, there are two paths for a general LLM, i.e. an LLM straight out of high school, to get 'higher education':\n",
       "\n",
       "* Fine Tuning\n",
       "* Retrieval Augmented Generation (RAG)\n",
       "\n",
       "![alt text](pics/LLMStudent.jpg 'LLMStudent.jpg')\n",
       "\n",
       "<i>Figure 1.1. </i>\n",
       "\n",
       "\n",
       "\n",
       "I am a retired university teacher.  One might argue that it was natural for me to become interested in providing 'university' training to the LLMs.  I mentioned this interest in March 2023 but was not sure yet how to go about it.  I have been watching the progress in the field since then.  A number of tools have been proposed but were lacking in one way or another. I did not want to invest my time in a tool that would not be around long.  The situation has now changed. I am happy to say that the OpenAI offerings early November provides a path for people like me to develop tools to train LLMs to competence levels of a college graduate. They are not perfect but good enough to provide a starting point.\n",
       "\n",
       "I am planning to use Retrieval Augmented Generation (RAG) to do this. Before I justify this choice, I should give a very brief description of the two methods mentioned above.\n",
       "\n",
       "## Fine Tuning ##\n",
       "\n",
       "This method is a repetition of the initial training of the LLM.  Remember that the model is already trained on a large corpus of text.  The fine tuning is done on a smaller corpus of text that is specific to the task at hand. In human education terms, this is similar to recitation learning like in islamic madrasas.  The madrasa students keeo reciting religious texts without necessarily understanding context.  Similarly, in fine-tuning, the LLM is given a body of text in a specific domain and is trained to tease out the probabilistic relations connecting different words to each other in this specific domain, which may be slightly different from the original relations developed using an entire corpus of general internet and other sources. As in its original initial training, there is no contextual knowledge relations here just probabilities. \n",
       "\n",
       "Either the entire parameter set (weights and biases) of the LLM is fine tuned or only the last layer is fine tuned. The fine-tuning starts with unsupervised learning, which is usually reinforced by human feedback.\n",
       "\n",
       "I decided not to use fine-tuning because at the end the answer will still be probabilistic and not contextual.A probalistic answer may be adequate in non-numerical fields such as law but it is totally unacceptable in say engineering where categorical answers are needed and if a numerical response is asked for, it should be accurate with reliabilities exceeding 99%\n",
       "\n",
       "In a way, it is good that there are good reasons for me not to pick fine-tuning as my method of choice because I do not have access to the computing power needed to do fine-tuning. It is possible to use OpenaAI API but this would be expensive.  I also think some heuristic methods need to be used in addition to running fine-tuning through the API and I do not think this would be possible when using the OpenAI as a black box.\n",
       "\n",
       "## Retrieval Augmented Generation (RAG) ##\n",
       "\n",
       "If fine-tuning is like recitation learning, RAG is like contextual learning.  In RAG, there is a retriever program between the user and the LLM.  When the user asks a question, the retriever program retrieves the most relevant text from a corpus of text and feeds it to the LLM.  The LLM then generates a response.  The response is then fed back to the retriever program to determine if the response is relevant to the question.  If it is not, the retriever program retrieves another text from the corpus and the process is repeated until a relevant response is obtained.\n",
       "\n",
       "Most of the current interest in RAG is in building company chatbots where the corpus of text is the company's knowledge base.  The retriever program is usually a search engine. The corpus of text precedes RAG and is developed independent of the RAG effort.\n",
       "\n",
       "This is not my interest.  I am interested in developing a corpus of text that is specific to a specific area in which I have knowledge.  In other words, I approach RAG like writing a textbook for a course.  The difference is that writing a textbook for a LLM is probably different from writing.\n",
       "\n",
       "Let me give an example.  Suppose that I am an expert on Australian wildlife and I want to develop a chatbot that people can ask questions about Australian wildlife. I will first organise my knowledge in series of text files:\n",
       "\n",
       "![alt text](pics/corpus.jpg 'corpus.jpg')\n",
       "\n",
       "<i>Figure 1.2. </i>\n",
       "\n",
       "\n",
       "\n",
       "Then I will develop a retriever program that will retrieve the most relevant text file from the corpus and feed it to the LLM.  The LLM will then generate a response.  The response is then fed back to the retriever program to determine if the response is relevant to the question.  If it is not, the retriever program retrieves another text file from the corpus and the process is repeated until a relevant response is obtained.\n",
       "\n",
       "![alt text](pics/ragprocess.jpg 'ragprocess.jpg')\n",
       "\n",
       "<i>Figure 1.3. </i>\n",
       "\n",
       "\n",
       "\n",
       "### How does Retriever Program work? ###\n",
       "\n",
       "We cannot rely on a simple text matching search because the user may not the same words as in the corpus. Therefore, we need to use a semantic search.  To do semantic search, both the corpus text and the  user's question must be converted to vectors.  The vectors are then compared to each other.  These vectors are called embeddings.  The embeddings are generated by the LLM.  The LLM is trained to generate embeddings that are similar to each other if the text is similar.\n",
       "\n",
       "In summary, the beginning place is the embeddings.  We have to understand how embeddings work so that we can try different styles in writing our corpus text that will generate the most relevant embeddings.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SECTION+=1\n",
    "SECTION=1\n",
    "MD=mdx(Chapter, SECTION, TOC[SECTION-1])\n",
    "#\n",
    "MD.write(\"In March 2023, [I wrote in my blog](https://halimgur.substack.com/p/competent-intelligence-is-here-will) that the SOA LLMs were like high-school graduates:\\n\\n\\\n",
    "* Knows how to read and write\\n\\\n",
    "* Thinks they know everything\\n\\n\")\n",
    "MD.write(\"When you ask them a question you always get an answer because, if they do not know the answer, they would make it up\\n\\n\")\n",
    "MD.write(\"In March 2023, I said that LLMs were not yet competent enough to deliver professional functions such as engineering. \\\n",
    "A competent intelligence would be equivalent to a college graduate.  GPT-4 or even a future GPT-5 would have to be further trained to get there.  \\\n",
    "Today, there are two paths for a general LLM, i.e. an LLM straight out of high school, to get 'higher education':\\n\\n\\\n",
    "* Fine Tuning\\n\\\n",
    "* Retrieval Augmented Generation (RAG)\\n\\n:::3|LLMStudent.jpg::\\n\\n\\\n",
    "I am a retired university teacher.  One might argue that it was natural for me to become interested in providing 'university' training to the LLMs.  I mentioned \\\n",
    "this interest in March 2023 but was not sure yet how to go about it.  I have been watching the progress in the field since then.  A number of tools have been proposed \\\n",
    "but were lacking in one way or another. I did not want to invest my time in a tool \\\n",
    "that would not be around long.  The situation has now changed. \\\n",
    "I am happy to say that the OpenAI offerings early November provides a path for people like me to develop tools to \\\n",
    "train LLMs to competence levels of a college graduate. They are not perfect but good enough to provide a starting point.\\n\\n\\\n",
    "I am planning to use Retrieval Augmented Generation (RAG) to do this. Before I justify this choice, I should give \\\n",
    "a very brief description of the two methods mentioned above.\\n\\n\\\n",
    "## Fine Tuning ##\\n\\n\\\n",
    "This method is a repetition of the initial training of the LLM.  Remember that the model is already trained on \\\n",
    "a large corpus of text.  The fine tuning is done on a smaller corpus of text that is specific to the task at hand. \\\n",
    "In human education terms, this is similar to recitation learning like in islamic madrasas.  The madrasa students keeo \\\n",
    "reciting religious texts without necessarily understanding context.  Similarly, in fine-tuning, the LLM is \\\n",
    "given a body of text in a specific domain and is trained to tease out the probabilistic relations connecting \\\n",
    "different words to each other in this specific domain, which may be slightly different from the original relations \\\n",
    "developed using an entire corpus of general internet and other sources. As in its original initial training, there is \\\n",
    "no contextual knowledge relations here just probabilities. \\n\\n\\\n",
    "Either the entire parameter set (weights and biases) of the LLM is fine tuned or only the last layer is fine tuned. \\\n",
    "The fine-tuning starts with unsupervised learning, which is usually reinforced by human feedback.\\n\\n\\\n",
    "I decided not to use fine-tuning because at the end the answer will still be probabilistic and not contextual.\\\n",
    "A probalistic answer may be adequate in non-numerical fields such as law but it is totally unacceptable \\\n",
    "in say engineering where categorical answers are needed and if a numerical response is asked for, \\\n",
    "it should be accurate with reliabilities exceeding 99%\\n\\n\")\n",
    "MD.write(\"In a way, it is good that there are good reasons for me not to pick fine-tuning as my method \\\n",
    "of choice because I do not have access to the computing power needed to do fine-tuning. It is possible \\\n",
    "to use OpenaAI API but this would be expensive.  I also think some heuristic methods need to be used in addition to \\\n",
    "running fine-tuning through the API and I do not think this would be possible when using the OpenAI as a black box.\\n\\n\")\n",
    "MD.write(\"## Retrieval Augmented Generation (RAG) ##\\n\\n\")\n",
    "MD.write(\"If fine-tuning is like recitation learning, RAG is like contextual learning.  In RAG, there is \\\n",
    "a retriever program between the user and the LLM.  When the user asks a question, the retriever program \\\n",
    "retrieves the most relevant text from a corpus of text and feeds it to the LLM.  The LLM then generates \\\n",
    "a response.  The response is then fed back to the retriever program to determine if the response is \\\n",
    "relevant to the question.  If it is not, the retriever program retrieves another text from the corpus \\\n",
    "and the process is repeated until a relevant response is obtained.\\n\\n\\\n",
    "Most of the current interest in RAG is in building company chatbots where the corpus of text is the \\\n",
    "company's knowledge base.  The retriever program is usually a search engine. The corpus of text \\\n",
    "precedes RAG and is developed independent of the RAG effort.\\n\\n\\\n",
    "This is not my interest.  I am interested in developing a corpus of text that is specific to a \\\n",
    "specific area in which I have knowledge.  In other words, I approach RAG like writing a textbook \\\n",
    "for a course.  The difference is that writing a textbook for a LLM is probably different from writing.\\n\\n\\\n",
    "Let me give an example.  Suppose that I am an expert on Australian wildlife and I want to \\\n",
    "develop a chatbot that people can ask questions about Australian wildlife. I will first organise my knowledge \\\n",
    "in series of text files:\\n\\n:::3|corpus.jpg::\\n\\n\\\n",
    "Then I will develop a retriever program that will retrieve the most relevant text file from the corpus \\\n",
    "and feed it to the LLM.  The LLM will then generate a response.  The response is then fed back to the \\\n",
    "retriever program to determine if the response is relevant to the question.  If it is not, the retriever \\\n",
    "program retrieves another text file from the corpus and the process is repeated until a relevant response \\\n",
    "is obtained.\\n\\n:::3|ragprocess.jpg::\\n\\n\")\n",
    "MD.write(\"### How does Retriever Program work? ###\\n\\n\")\n",
    "MD.write(\"We cannot rely on a simple text matching search because the user may not the same words as in the corpus. \\\n",
    "Therefore, we need to use a semantic search.  To do semantic search, both the corpus text and the  \\\n",
    "user's question must be converted to vectors.  The vectors are then compared to each other.  These vectors are \\\n",
    "called embeddings.  The embeddings are generated by the LLM.  The LLM is trained to generate embeddings \\\n",
    "that are similar to each other if the text is similar.\\n\\n\\\n",
    "In summary, the beginning place is the embeddings.  We have to understand how embeddings work so that we can \\\n",
    "try different styles in writing our corpus text that will generate the most relevant embeddings.\\n\\n\")\n",
    "MD.write(\"\\n\\n\")\n",
    "#\n",
    "md(MD.out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Please Join Me #\n",
       "\n",
       "As you can see I am not trying to develop new programs to do RAG.  I wil use existing programs but I will experiment with different styles of writing the corpus text to see which style generates the most relevant embeddings. I will also experiment with different ways of retrieving the most relevant text from the corpus.  I suspect there will be room for some heuristics here.\n",
       "\n",
       "If you have an area of expertise which you would like to share with the world using this new technology, please join me.  I will be regularly (hopefully fortnightly) posting my progress on Substack.  I will also be posting my code on Github.  I will be using Python and the OpenAI API. In addition, I created an X (formerly known as Twitter) Community Group '[Building AI Tutors](https://twitter.com/i/communities/1727552258454474973)'. If you are interested, please join the group.  I will be posting my progress there as well.\n",
       "\n",
       "You do not need to be an experienced Python programmer but some knowledge of Python will be helpful.  Velow I list the steps you need to take to join me and explain how to go about it:\n",
       "\n",
       "* Install VS Code from [https://code.visualstudio.com/download](https://code.visualstudio.com/download).  This is a free code editor and development environment.  It is the tool I am using therefore should be able to help you if you have problems.\n",
       "* Install Python from [https://www.python.org/downloads/](https://www.python.org/downloads/).  At the time I started this notebook,\n",
       "  * I had `python 3.10.11` installed on my Windows computer.\n",
       "  * Python 3.11.1 on my Mac\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SECTION+=1\n",
    "SECTION=2\n",
    "MD=mdx(Chapter, SECTION, TOC[SECTION-1])\n",
    "MD.write(\"As you can see I am not trying to develop new programs to do RAG.  I wil use existing programs but I will \\\n",
    "experiment with different styles of writing the corpus text to see which style generates the most relevant embeddings. \\\n",
    "I will also experiment with different ways of retrieving the most relevant text from the corpus.  I suspect there will be \\\n",
    "room for some heuristics here.\\n\\n\")\n",
    "MD.write(\"If you have an area of expertise which you would like to share with the world using this \\\n",
    "new technology, please join me.  I will be regularly (hopefully fortnightly) posting my progress \\\n",
    "on Substack.  I will also be posting my code on Github.  I will be using Python and the OpenAI API. In addition, I created \\\n",
    "an X (formerly known as Twitter) Community Group '[Building AI Tutors](https://twitter.com/i/communities/1727552258454474973)'. If you \\\n",
    "are interested, please join the group.  I will be posting my progress there as well.\\n\\n\")\n",
    "MD.write(\"You do not need to be an experienced Python programmer but some knowledge of Python will be helpful.  Velow I list the steps \\\n",
    "you need to take to join me and explain how to go about it:\\n\\n\\\n",
    "* Install VS Code from [https://code.visualstudio.com/download](https://code.visualstudio.com/download).  \\\n",
    "This is a free code editor and development environment.  It is the tool I am using therefore should be \\\n",
    "able to help you if you have problems.\\n\\\n",
    "* Install Python from [https://www.python.org/downloads/](https://www.python.org/downloads/).  \\\n",
    "At the time I started this notebook,\\n\\\n",
    "  * I had `python 3.10.11` installed on my Windows computer.\\n\\\n",
    "  * Python 3.11.1 on my Mac\\n\\n\")\n",
    "MD.write(\"\\n\\n\")\n",
    "#\n",
    "md(MD.out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Install Python #\n",
       "\n",
       "Download and install python from [https://www.python.org/downloads/] if you do not have it already.  At the time I started this notebook,\n",
       "\n",
       "* I had `python 3.10.11` installed on my Windows computer.\n",
       "* Python 3.11.1 on my Mac\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SECTION+=1\n",
    "MD=mdx(Chapter, SECTION, TOC[SECTION-1])\n",
    "#\n",
    "MD.write(\"Download and install python from [https://www.python.org/downloads/] if you do not have it already.  At the time I started this notebook,\\n\\n\\\n",
    "* I had `python 3.10.11` installed on my Windows computer.\\n\\\n",
    "* Python 3.11.1 on my Mac\")\n",
    "MD.write(\"\\n\\n\")\n",
    "#\n",
    "md(MD.out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Set up Virtual Environment #\n",
       "\n",
       "It is good practice to have a virtual environment dedicated to the project. This is to ensure that the project has all the required libraries and versions. The virtual environment must be created as the first action after creating the file before installing any libraries.\n",
       "\n",
       "I am running this Jupyter notebook in VS Code.  If you are using another editor, your method will be different.  In VS Code, I create the virtual environment using `CTRL`+`SHIFT`+`P` $\\rightarrow$  `Pyton: Create Environment`$\\rightarrow$ Pick the `venv` option.\n",
       "\n",
       "This creates a `.venv` folder in the project folder.  The `.venv` folder is where the virtual environment is. Here are the contents of the `pyvenv.cfg` generated under `.venv` :\n",
       "\n",
       "\n",
       "\n",
       "### WINDOWS ###\n",
       "\n",
       "home = `C:\\Users\\e4hgurge\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0`\n",
       "\n",
       "include-system-site-packages = false\n",
       "\n",
       "version = 3.10.11\n",
       "\n",
       "\n",
       "\n",
       "### MAC ###\n",
       "\n",
       "home = /Library/Frameworks/Python.framework/Versions/3.11/bin\n",
       "\n",
       "include-system-site-packages = false\n",
       "\n",
       "version = 3.11.1\n",
       "\n",
       "executable = /Library/Frameworks/Python.framework/Versions/3.11/bin/python3.11\n",
       "\n",
       "command = /usr/local/bin/python3 -m venv /Users/Halim/openAI/openAI_first/.venv\n",
       "\n",
       "**IMPORTANT**\n",
       "\n",
       "Every time you start VS Code, you must make sure you are running in the virtual environment. The environment can be seen in the upper right corner of the VS Code window.  If you are not running in the virtual environment, click there on the upper right corner on the displayed Python version and select the virtual environment.\n",
       "\n",
       "Sometime, VSCode tells me that running under `venv` requires the `ipykernel`. It will ask me whether to install it.  I always accept the offer click on `Install`.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SECTION+=1\n",
    "MD=mdx(Chapter, SECTION, TOC[SECTION-1])\n",
    "#\n",
    "MD.write(\"It is good practice to have a virtual environment dedicated to the project. This is to ensure that the project \\\n",
    "has all the required libraries and versions. The virtual environment must be created \\\n",
    "as the first action after creating the file before installing any libraries.\\n\\n\")\n",
    "MD.write(\"I am running this Jupyter notebook in VS Code.  If you are using another editor, \\\n",
    "your method will be different.  In VS Code, I create the virtual environment using `CTRL`+`SHIFT`+`P` $\\\\rightarrow$  \\\n",
    "`Pyton: Create Environment`$\\\\rightarrow$ Pick the `venv` option.\\n\\n\\\n",
    "This creates a `.venv` folder in the project folder.  The `.venv` folder is where the virtual environment is. \\\n",
    "Here are the contents of the `pyvenv.cfg` generated under `.venv` :\\n\\n\\\n",
    "\\n\\n\\\n",
    "### WINDOWS ###\\n\\n\\\n",
    "home = `C:\\\\Users\\e4hgurge\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0`\\n\\n\\\n",
    "include-system-site-packages = false\\n\\n\\\n",
    "version = 3.10.11\\n\\n\\\n",
    "\\n\\n\\\n",
    "### MAC ###\\n\\n\\\n",
    "home = /Library/Frameworks/Python.framework/Versions/3.11/bin\\n\\n\\\n",
    "include-system-site-packages = false\\n\\n\\\n",
    "version = 3.11.1\\n\\n\\\n",
    "executable = /Library/Frameworks/Python.framework/Versions/3.11/bin/python3.11\\n\\n\\\n",
    "command = /usr/local/bin/python3 -m venv /Users/Halim/openAI/openAI_first/.venv\\n\\n\\\n",
    "**IMPORTANT**\\n\\n\\\n",
    "Every time you start VS Code, you must make sure you are running in the virtual environment. The environment can be seen \\\n",
    "in the upper right corner of the VS Code window.  If you are not running in the virtual environment, click there \\\n",
    "on the upper right corner on the \\\n",
    "displayed Python version and select the virtual environment.\\n\\n\\\n",
    "Sometime, VSCode tells me that running under `venv` requires the `ipykernel`. It will ask me whether to install it.  I always accept the offer click on `Install`.\\n\\n\\\n",
    "\")\n",
    "\n",
    "md(MD.out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Establish OpenAI Credentials #\n",
       "\n",
       "You need to have an `openai` account to place python calls to `openai` platform (openAI API).  This means openai will charge you for the usage.  I have an openai account where I set the monthly limit to 120 dollars (Australian).  The monthly limit is necessary.  Otherwise, you can make a coding error for example that will make your program call openai 100 times a second and run a huge bill.  So far, my monthly bill has not reached $1 yet.  So for the stuff we are doing here, the charge is trivial.\n",
       "\n",
       "You must have your own account to run this notebook.\n",
       "\n",
       "To get an openai account, go to [https://beta.openai.com/](https://beta.openai.com/) and follow the instructions.  Once you have an account, you need to get an API key.  To get the API key, click on your name on the upper right corner of the screen.  Then click on `My Account`.  Then click on `API Keys` on the left side of the screen.  Then click on `Create API Key` on the right side of the screen.  Then copy the API key and paste it as an environment variable into your configuration file as described below.\n",
       "\n",
       "**_Setting OpenAPI credentials on Windows Computer_**\n",
       "\n",
       "Define the environment variable `OPENAI_API_KEY` by using the `setx` command in Windows:\n",
       "\n",
       "```\n",
       "setx OPENAI_API_KEY 'my-api-key-here'\n",
       "```\n",
       "\n",
       "Use `setx` not `set` to make sure that the OPENAI_API_KEY will be available globally and persistently.  Otherwise (i.e. if using `set`), it would be available only for the current session.\n",
       "\n",
       "\n",
       "\n",
       "To check if it is set correctly:\n",
       "\n",
       "* Close the terminal window\n",
       "* Open a new terminal window\n",
       "* Enter the command: `echo %OPENAI_API_KEY%`.  This should display my OPENAI access key.  If it is not set, it will simply echo the string `%OPENAI_API_KEY%`\n",
       "\n",
       "**_On `macOS`_**\n",
       "\n",
       "* Open Terminal (use spotlight, search for `Terminal.app`)\n",
       "* Edit Bash profile using `nano ~/.zshrc`\n",
       "*  * `nano` is a command-line text editor.  It is a simple interface for editing text files in the terminal\n",
       "*  * `~` is the shortcut for user's home directory\n",
       "*  * `.zshrc` is the configuration file for the Z shell, which is the default shell in macOS starting with Catalina (macOS 10.15) -- My MAC runs sonoma 14.0\n",
       "*  Add the line `export OPENAI_API_KEY='my-api-key-here'`\n",
       "*  Use CTRL+O to save changes and CTRL+X to exit `nano`\n",
       "*  Load the profile using `source ~/.zshrc`\n",
       "*  Verify the set-up by typing `echo $OPENAI_API_KEY`\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SECTION+=1\n",
    "MD=mdx(Chapter, SECTION, TOC[SECTION-1])\n",
    "#\n",
    "MD.write(\"You need to have an `openai` account to place python calls to `openai` platform (openAI API).  This means openai will charge you for the usage.  \\\n",
    "I have an openai account where I set the monthly limit to 120 dollars (Australian).  The monthly limit is necessary.  Otherwise, you can make a coding error for example \\\n",
    "that will make your program call openai 100 times a second and run a huge bill.  So far, my monthly bill has not reached $1 yet.  \\\n",
    "So for the stuff we are doing here, the charge is trivial.\\n\\n\\\n",
    "You must have your own account to run this notebook.\\n\\n\\\n",
    "\")\n",
    "MD.write(\"To get an openai account, go to [https://beta.openai.com/](https://beta.openai.com/) and follow the instructions.  \\\n",
    "Once you have an account, you need to get an API key.  To get the API key, click on your name on the upper right corner of the screen.  \\\n",
    "Then click on `My Account`.  Then click on `API Keys` on the left side of the screen.  Then click on `Create API Key` on the right side of the screen.  \\\n",
    "Then copy the API key and paste it as an environment variable into your configuration file as described below.\\n\\n\")\n",
    "#\n",
    "MD.write(\"**_Setting OpenAPI credentials on Windows Computer_**\\n\\n\\\n",
    "Define the environment variable `OPENAI_API_KEY` by using the `setx` command in Windows:\\n\\n\\\n",
    "```\\n\\\n",
    "setx OPENAI_API_KEY 'my-api-key-here'\\n\\\n",
    "```\\n\\n\\\n",
    "Use `setx` not `set` to make sure that the OPENAI_API_KEY will be available globally and persistently.  Otherwise (i.e. if using `set`), it would be available only for the current session.\\n\\n\\\n",
    "\\n\\n\\\n",
    "To check if it is set correctly:\\n\\n\\\n",
    "* Close the terminal window\\n\\\n",
    "* Open a new terminal window\\n\\\n",
    "* Enter the command: `echo %OPENAI_API_KEY%`.  This should display my OPENAI access key.  If it is not set, it will simply echo the string `%OPENAI_API_KEY%`\\n\\n\\\n",
    "**_On `macOS`_**\\n\\n\\\n",
    "* Open Terminal (use spotlight, search for `Terminal.app`)\\n\\\n",
    "* Edit Bash profile using `nano ~/.zshrc`\\n\\\n",
    "*  * `nano` is a command-line text editor.  It is a simple interface for editing text files in the terminal\\n\\\n",
    "*  * `~` is the shortcut for user's home directory\\n\\\n",
    "*  * `.zshrc` is the configuration file for the Z shell, which is the default shell in macOS starting with Catalina (macOS 10.15) -- My MAC runs sonoma 14.0\\n\\\n",
    "*  Add the line `export OPENAI_API_KEY='my-api-key-here'`\\n\\\n",
    "*  Use CTRL+O to save changes and CTRL+X to exit `nano`\\n\\\n",
    "*  Load the profile using `source ~/.zshrc`\\n\\\n",
    "*  Verify the set-up by typing `echo $OPENAI_API_KEY`\\n\\n\\\n",
    "\")\n",
    "MD.write(\"\\n\\n\")\n",
    "#\n",
    "md(MD.out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# What is an embedding? #\n",
       "\n",
       "In Large Language Models (LLMs) an embedding is a list of numbers, which are between -1 and +1.  This list is referred to as the embedding vector. The embedding vector represents the features of the text string.  The embedding vector is created by a neural network.  The neural network is trained on a large corpus of text.\n",
       "\n",
       "You can think of the embedding vector as a point in a high dimensional space.  The number of dimensions is the number of numbers in the embedding vector.\n",
       "\n",
       "Let me use an example from everyday life to explain the concept of embedding.  Suppose you are a real estate agent.  You have a list of houses for sale.  Each house has a number of features such as number of bedrooms, number of bathrooms, size of the land, size of the house, etc.  You can think of each house as a point in a high dimensional space.  The number of dimensions is the number of features.  The features are the numbers that describe the house.  The features are the coordinates of the point in the high dimensional space.  The coordinates are the numbers that describe the house.  The coordinates are the features of the house.  The coordinates are the embedding vector of the house.\n",
       "\n",
       "### Numerical Example ###\n",
       "\n",
       "In another example, assume you are the CEO of a very large company with hundreds of branches around the country.  Your HR Department wants to give an award to the best branch in terms of the human resources.  \n",
       "\n",
       "How do they do that?  They ask each branch nominate four of their employees who will write an essay where they describe the mission of the company and their place in it, and also phone interview them.\n",
       "\n",
       "The employees are assessed on a number of criteria including their education, customer references, the quality of their essay, communications skills.\n",
       "\n",
       "From the Cairns branch, for example, the nominees are John, Pascal, Emin, and Rachel.  The HR Department will create an embedding vector for each nominee.  \n",
       "\n",
       "||John|Pascal|Emin|Rachel|Cairns|\n",
       "|--|--|--|--|--|--|\n",
       "|Highest Degree   |2|2|3|2|2.25|\n",
       "|Customer Reference quality|5|7|9|6|6.75|\n",
       "|Writing skills.  |6|7|9|7|7.25|\n",
       "|Understanding of the requirements|4|5|3|6|4.50|\n",
       "|Verbal communications|6|6|5|8|6.25|\n",
       "\n",
       "We can refer to this table as our embeddings in this instance.  Each column is an embedding vector:\n",
       "\n",
       "* $\\overrightarrow{\\text{John}}=\\{2,5,6,4,6\\}$;\n",
       "* $\\overrightarrow{\\text{Pascal}}=\\{2,7,7,5,6\\}$;\n",
       "* $\\overrightarrow{\\text{Emin}}=\\{3,9,9,3,5\\}$;\n",
       "* $\\overrightarrow{\\text{Rachel}}=\\{2,6,7,6,8\\}$.\n",
       "\n",
       "Last column is the average for the Cairns branch.  We can refer to it as the embedding vector for Cairns:\n",
       "\n",
       "* $\\overrightarrow{\\text{Cairns}}=\\{2.25,6.75,7.25,4.50,6.25\\}$.\n",
       "\n",
       "**Some observations on Embedding vectors:**\n",
       "\n",
       "* For a given embedding scheme, the number of dimensions is fixed.  In the example above, the number of dimensions is 5.\n",
       "* The embedding vector is a list of numbers.  In the example above, the embedding vector is a list of 5 numbers.\n",
       "* The embedding vector for one text string is different from the embedding vector for another text string.  In the example above, the embedding vector for John is different from the embedding vector for Pascal.\n",
       "* The embedding vector for a text string is the same every time it is created.  In the example above, the embedding vector for John is the same if another HR officer calculates it.  It will remain to be the same unless the rules (the model) change.\n",
       "* The embedding vector for one string and the embedding vector for an ensemble of strings have the same number of dimensions\n",
       "* In the above example, the magnitude of the embedding vectors is not important.  What is important is the relative magnitude of the numbers in the embedding vector.\n",
       "* In the above example, the embedding vectors are not normalised.  Normalisation is not necessary for the embedding vectors to be useful.  However, normalisation is necessary for the embedding vectors to be comparable.  In the above example, the embedding vectors are not comparable.\n",
       "* Text embedding vectors used in LLMs are comparable because they are normalised.  The normalisation is done by the neural network that creates the embedding vectors.\n",
       "\n",
       "### Embedding vectors in LLMs ###\n",
       "\n",
       "As an example, I computed the embedding vectors and the magnitudes of those vectors for the following text strings:\n",
       "\n",
       "|String|Embedding Vector Magnitude|Embedding Vector Length\n",
       "|--|--|---|\n",
       "|There are four sides to a square.|1.00000|1536|\n",
       "|There are three sides to a triangle.|1.00000|1536|\n",
       "|There are five sides to a pentagon.|1.00000|1536|\n",
       "|In a footbal game, there are eleven players on each team.|1.00000|1536|\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SECTION+=1\n",
    "MD=mdx(Chapter, SECTION, TOC[SECTION-1])\n",
    "#\n",
    "MD.write(\"In Large Language Models (LLMs) an embedding is a list of numbers, which are between -1 and +1.  This list is referred to as the embedding vector. \\\n",
    "The embedding vector represents the features of the text string.  The embedding vector is created by a neural network.  The neural network is trained on a large \\\n",
    "corpus of text.\\n\\n\")\n",
    "#\n",
    "MD.write(\"You can think of the embedding vector as a point in a high dimensional space.  The number of dimensions is the number of numbers in the embedding vector.\\n\\n\")\n",
    "MD.write(\"Let me use an example from everyday life to explain the concept of embedding.  Suppose you are a real estate agent.  You have a list of houses for sale.  \\\n",
    "Each house has a number of features such as number of bedrooms, number of bathrooms, size of the land, size of the house, etc.  You can think of each house as a point \\\n",
    "in a high dimensional space.  The number of dimensions is the number of features.  The features are the numbers that describe the house.  The features are the \\\n",
    "coordinates of the point in the high dimensional space.  The coordinates are the numbers that describe the house.  The coordinates are the features of the house.  \\\n",
    "The coordinates are the embedding vector of the house.\\n\\n\")\n",
    "MD.write(\"### Numerical Example ###\\n\\n\")\n",
    "MD.write(\"In another example, assume you are the CEO of a very large company with hundreds of branches around the country.  Your HR Department wants to give an award \\\n",
    "to the best branch in terms of the human resources.  \\n\\nHow do they do that?  They ask each branch nominate four of their employees \\\n",
    "who will write an essay where \\\n",
    "they describe the mission of the company and their place in it, and also phone interview them.\\n\\n\\\n",
    "The employees are assessed on a number of criteria including their education, customer references, the quality of their essay, communications skills.\\n\\n\\\n",
    "From the Cairns branch, for example, the nominees are John, Pascal, Emin, and Rachel.  The HR Department will create an embedding vector for each nominee.  \\n\\n\\\n",
    "||John|Pascal|Emin|Rachel|Cairns|\\n\\\n",
    "|--|--|--|--|--|--|\\n\\\n",
    "|Highest Degree   |2|2|3|2|2.25|\\n\\\n",
    "|Customer Reference quality|5|7|9|6|6.75|\\n\\\n",
    "|Writing skills.  |6|7|9|7|7.25|\\n\\\n",
    "|Understanding of the requirements|4|5|3|6|4.50|\\n\\\n",
    "|Verbal communications|6|6|5|8|6.25|\\n\\n\\\n",
    "\")\n",
    "MD.write(\"We can refer to this table as our embeddings in this instance.  Each column is an embedding vector:\\n\\n\\\n",
    "* $\\\\overrightarrow{\\\\text{John}}=\\{2,5,6,4,6\\}$;\\n\\\n",
    "* $\\\\overrightarrow{\\\\text{Pascal}}=\\{2,7,7,5,6\\}$;\\n\\\n",
    "* $\\\\overrightarrow{\\\\text{Emin}}=\\{3,9,9,3,5\\}$;\\n\\\n",
    "* $\\\\overrightarrow{\\\\text{Rachel}}=\\{2,6,7,6,8\\}$.\\n\\n\\\n",
    "Last column is the average for the Cairns branch.  We can refer to it as the embedding vector for Cairns:\\n\\n\\\n",
    "* $\\\\overrightarrow{\\\\text{Cairns}}=\\{2.25,6.75,7.25,4.50,6.25\\}$.\")\n",
    "MD.write(\"\\n\\n**Some observations on Embedding vectors:**\\n\\n\\\n",
    "* For a given embedding scheme, the number of dimensions is fixed.  In the example above, the number of dimensions is 5.\\n\\\n",
    "* The embedding vector is a list of numbers.  In the example above, the embedding vector is a list of 5 numbers.\\n\\\n",
    "* The embedding vector for one text string is different from the embedding vector for another text string.  In the example above, the embedding vector for John is different from the embedding vector for Pascal.\\n\\\n",
    "* The embedding vector for a text string is the same every time it is created.  In the example above, the embedding vector for John is the same if another \\\n",
    "HR officer calculates it.  It will remain to be the same unless the rules (the model) change.\\n\\\n",
    "* The embedding vector for one string and the embedding vector for an ensemble of strings have the same number of dimensions\\n\\\n",
    "* In the above example, the magnitude of the embedding vectors is not important.  What is important is the relative magnitude of the numbers in the embedding vector.\\n\\\n",
    "* In the above example, the embedding vectors are not normalised.  Normalisation is not necessary for the embedding vectors to be useful.  However, normalisation \\\n",
    "is necessary for the embedding vectors to be comparable.  In the above example, the embedding vectors are not comparable.\\n\\\n",
    "* Text embedding vectors used in LLMs are comparable because they are normalised.  The normalisation is done by the neural network that creates the embedding vectors.\\n\\n\")\n",
    "MD.write(\"### Embedding vectors in LLMs ###\\n\\n\")\n",
    "MD.write(\"As an example, I computed the embedding vectors and the magnitudes of those vectors for the following text strings:\\n\\n\")\n",
    "MD.write(\"|String|Embedding Vector Magnitude|Embedding Vector Length\\n\\\n",
    "|--|--|---|\\n\")\n",
    "for i,s in enumerate([TextString1, TextString2, TextString3, TextString4]):\n",
    "    MD.write(\"|%s|%.5f|%d|\\n\"%(s, Magnitudes[i], len(Emvectors[i])))\n",
    "MD.write(\"\\n\\n\")\n",
    "#\n",
    "md(MD.out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
