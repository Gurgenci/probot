{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from ute import init_openai, read_page_list, rdweb, embedpagelist, sepstr\n",
    "from ute import SEP, get_embedding, cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Define the md() function to display markdown text\n",
    "from IPython.display import display, Markdown\n",
    "def md(s):\n",
    "    display(Markdown(s))\n",
    "\n",
    "\n",
    "(Client, LLM)=init_openai()\n",
    "webfolder=\"general\"\n",
    "datafolder=\"data\"+\"/\"+webfolder+\"/\"\n",
    "DataFile=datafolder+\"data\"+\".txt\"\n",
    "TextListFile=datafolder+webfolder+\"_textlist.txt\"\n",
    "UploadListFile=datafolder+webfolder+\"_uploadlist.txt\"\n",
    "UploadIDFile=datafolder+webfolder+\"_uploadid.txt\"\n",
    "EmbeddingFile=datafolder+webfolder+\"_embedding.npz\"\n",
    "#\n",
    "TextSimilarity=None\n",
    "topN=None\n",
    "N=3\n",
    "#\n",
    "Thread=None\n",
    "Run=None\n",
    "Messages=[]\n",
    "RunIDs=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|Contents|File Name|\n",
       "|--|--|\n",
       "|Web folder name|general|\n",
       "|Data folder name|data/general/|\n",
       "|Data file name|data/general/data.txt|\n",
       "|Text list file name|data/general/general_textlist.txt|\n",
       "|Upload list file name|data/general/general_uploadlist.txt|\n",
       "|Upload ID file name|data/general/general_uploadid.txt|\n",
       "|Embedding file name|data/general/general_embedding.npz|\n",
       "|Client|<openai.OpenAI object at 0x10d4f9190>|\n",
       "|LLM|text-embedding-ada-002|\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Variable name|Description|\n",
       "|--|--|\n",
       "|`webfolder`|Web folder name|\n",
       "|`datafolder`|Data folder name|\n",
       "|`DataFile`|Data file name|\n",
       "|`TextListFile`|Text list file name|\n",
       "|`UploadListFile`|Upload list file name|\n",
       "|`UploadIDFile`|Upload ID file name|\n",
       "|`EmbeddingFile`|Embedding file name|\n",
       "|`Client`|OpenAI client|\n",
       "|`LLM`|Language model|\n",
       "`UploadList`|List of file names to upload|\n",
       "`UploadID`|List of OpenAI file IDs (created when files are uploaded)|\n",
       "`TextList`|List of text files (as pairs of two lines, URL and title)|\n",
       "`Embedding`|Embedding matrix|\n",
       "`TextSimilarity`|Similarity vector|\n",
       "`topN`|`TextSimilarity` indices of the N best-matching pages|\n",
       "`N`|Number of best-matching pages to display|\n",
       "`Thread`|Current thread object|\n",
       "`Run`|Current run object|\n",
       "`Messages`|List of messages on the current thread|\n",
       "`RunIDs`|List of run IDs on the current thread|\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s=\"|Contents|File Name|\\n|--|--|\\n\\\n",
    "|Web folder name|\"+webfolder+\"|\\n\\\n",
    "|Data folder name|\"+datafolder+\"|\\n\\\n",
    "|Data file name|\"+DataFile+\"|\\n\\\n",
    "|Text list file name|\"+TextListFile+\"|\\n\\\n",
    "|Upload list file name|\"+UploadListFile+\"|\\n\\\n",
    "|Upload ID file name|\"+UploadIDFile+\"|\\n\\\n",
    "|Embedding file name|\"+EmbeddingFile+\"|\\n\\\n",
    "|Client|\"+str(Client)+\"|\\n\\\n",
    "|LLM|\"+str(LLM)+\"|\\n\\\n",
    "\\n\\n\"\n",
    "md(s)\n",
    "s=\"|Variable name|Description|\\n|--|--|\\n\\\n",
    "|`webfolder`|Web folder name|\\n\\\n",
    "|`datafolder`|Data folder name|\\n\\\n",
    "|`DataFile`|Data file name|\\n\\\n",
    "|`TextListFile`|Text list file name|\\n\\\n",
    "|`UploadListFile`|Upload list file name|\\n\\\n",
    "|`UploadIDFile`|Upload ID file name|\\n\\\n",
    "|`EmbeddingFile`|Embedding file name|\\n\\\n",
    "|`Client`|OpenAI client|\\n\\\n",
    "|`LLM`|Language model|\\n\\\n",
    "`UploadList`|List of file names to upload|\\n\\\n",
    "`UploadID`|List of OpenAI file IDs (created when files are uploaded)|\\n\\\n",
    "`TextList`|List of text files (as pairs of two lines, URL and title)|\\n\\\n",
    "`Embedding`|Embedding matrix|\\n\\\n",
    "`TextSimilarity`|Similarity vector|\\n\\\n",
    "`topN`|`TextSimilarity` indices of the N best-matching pages|\\n\\\n",
    "`N`|Number of best-matching pages to display|\\n\\\n",
    "`Thread`|Current thread object|\\n\\\n",
    "`Run`|Current run object|\\n\\\n",
    "`Messages`|List of messages on the current thread|\\n\\\n",
    "`RunIDs`|List of run IDs on the current thread|\\n\\\n",
    "\\n\\n\"\n",
    "md(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the text files locally and upload to OpenAI ##\n",
    "We upload the files to the file repository of the Client. They are not yet associated with any Thread.  A new thread is created after the Query is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the-requiem-for-a-dream-israels-untaken.txt because it has already been uploaded\n",
      "Skipping the-importance-of-elite-consensus.txt because it has already been uploaded\n",
      "Skipping openai-astounds-us-again.txt because it has already been uploaded\n",
      "Skipping why-did-elon-musk-buy-twitter.txt because it has already been uploaded\n",
      "Skipping elon-musk-the-spaceman.txt because it has already been uploaded\n",
      "Skipping let-us-talk-about-elon-musk.txt because it has already been uploaded\n",
      "Skipping despicable-acts-part-2.txt because it has already been uploaded\n",
      "Skipping despicable-deeds.txt because it has already been uploaded\n",
      "Skipping rogue-age-and-climate-change-unpredictable.txt because it has already been uploaded\n",
      "Skipping rogue-age-accessory-1-population.txt because it has already been uploaded\n",
      "Skipping rogue-renaissance-on-globe-with-upheavals.txt because it has already been uploaded\n",
      "Skipping the-great-stagnation-ends-but-for.txt because it has already been uploaded\n",
      "Skipping when-the-rivers-run-dry.txt because it has already been uploaded\n",
      "Skipping the-voice-referendum-in-australia.txt because it has already been uploaded\n",
      "Skipping conspiracy-theories-part-2.txt because it has already been uploaded\n",
      "Skipping conspiracy-theories-part-1.txt because it has already been uploaded\n",
      "Skipping how-many-more-ruins-in-great-britain.txt because it has already been uploaded\n",
      "Skipping while-watching-utopia-on-abc.txt because it has already been uploaded\n",
      "Skipping lying-oracles-and-the-anyone-and.txt because it has already been uploaded\n",
      "Skipping the-one-thing-necessary-for-the-triumph.txt because it has already been uploaded\n",
      "Skipping will-there-be-a-war.txt because it has already been uploaded\n",
      "Skipping how-do-we-select-our-information.txt because it has already been uploaded\n",
      "Skipping do-we-know-why-we-know-what-we-know.txt because it has already been uploaded\n",
      "Skipping does-layering-affect-heat-loss.txt because it has already been uploaded\n",
      "Skipping elections-in-turkey.txt because it has already been uploaded\n",
      "Skipping monster-wave-soon-to-hit-your-shore.txt because it has already been uploaded\n",
      "Skipping try-counting-carbons-not-calories.txt because it has already been uploaded\n",
      "Skipping bridge-fixture-for-five-players.txt because it has already been uploaded\n",
      "Skipping the-attainment-of-happiness.txt because it has already been uploaded\n",
      "Skipping competent-intelligence-is-here-will.txt because it has already been uploaded\n",
      "Skipping the-prodigal-bird-returns.txt because it has already been uploaded\n",
      "Skipping earthquake-thoughts-and-facts.txt because it has already been uploaded\n",
      "Skipping why-were-so-many-buildings-destroyed.txt because it has already been uploaded\n",
      "Skipping retirement-a-journey-of-continued.txt because it has already been uploaded\n",
      "No new files uploaded\n",
      "No new IDs uploaded\n"
     ]
    }
   ],
   "source": [
    "# The following function reads the names of the URLs already uploaded to OpenAI\n",
    "def read_upload_list(UploadListFile):\n",
    "    UploadList=[]\n",
    "    if os.path.exists(UploadListFile):\n",
    "        with open(UploadListFile, 'r') as f:\n",
    "            UploadList = f.read().splitlines()\n",
    "    return UploadList\n",
    "\n",
    "UploadList=read_upload_list(UploadListFile)\n",
    "\n",
    "# The following function reads the IDs of the URLs already uploaded to OpenAI\n",
    "def read_upload_id(UploadIDFile):\n",
    "    UploadID=[]\n",
    "    if os.path.exists(UploadIDFile):\n",
    "        with open(UploadIDFile, 'r') as f:\n",
    "            UploadID = f.read().splitlines()\n",
    "    return UploadID\n",
    "\n",
    "UploadID=read_upload_id(UploadIDFile)\n",
    "#\n",
    "# The following function extracts a file name from the URL.  The file name is\n",
    "# used to name the text file that will be uploaded to OpenAI\n",
    "# The file name is also used to check if the file has already been uploaded\n",
    "# Use the last part of the URL as the file name\n",
    "def get_file_name(url):\n",
    "    url_parts=url.split(\"/\")\n",
    "    filename=url_parts[-1]\n",
    "    return filename\n",
    "\n",
    "# Read the URL addresses to be read into text files from the file TextListFile\n",
    "# Ignore the URLs that have already been uploaded to OpenAI\n",
    "\n",
    "TextList=read_page_list(TextListFile)\n",
    "NewUploads=[]\n",
    "NewUploadIDs=[]\n",
    "for (itext,pair) in enumerate(TextList):\n",
    "    (url,title)=pair\n",
    "    txtfile=get_file_name(url)+\".txt\"\n",
    "    if txtfile in UploadList:\n",
    "        print(\"Skipping %s because it has already been uploaded\"%txtfile)\n",
    "        continue\n",
    "    # Read the text from the URL address and save into the text file\n",
    "    rdweb(url, datafolder+txtfile)\n",
    "    print(\"Uploading %s\"%txtfile, end=\"\")\n",
    "    File = Client.files.create(\n",
    "        file=open(datafolder+txtfile, \"rb\"),\n",
    "        purpose='assistants'\n",
    ")    \n",
    "    print(\" --- Uploaded\")\n",
    "    time.sleep(1)\n",
    "    NewUploads.append(txtfile)\n",
    "    NewUploadIDs.append(File.id)\n",
    "with open(UploadListFile, 'a') as f:\n",
    "    for item in NewUploads:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "if len(NewUploads)>0:\n",
    "    print(\"Updated %s (Names of the Uploaded Files)\"%UploadListFile)\n",
    "else:\n",
    "    print(\"No new files uploaded\")\n",
    "# Append the new IDs to the file UploadIDFile\n",
    "with open(UploadIDFile, 'a') as f:\n",
    "    for item in NewUploadIDs:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "if len(NewUploadIDs)>0:\n",
    "    print(\"Updated %s (OpenAI IDs of the uploaded files)\"%UploadIDFile)\n",
    "else:\n",
    "    print(\"No new IDs uploaded\")\n",
    "#\n",
    "UploadID=UploadID+NewUploadIDs\n",
    "UploadList=UploadList+NewUploads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Assistant ID from file or create new ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve Assistant ID: asst_LCdnKNcUtkxVg1aDoiQ9vHCX\n"
     ]
    }
   ],
   "source": [
    "# Check if the DataFile exists\n",
    "if os.path.isfile(DataFile):\n",
    "    # Read the Assistant ID from the file\n",
    "    f = open(DataFile, \"r\")\n",
    "    AssistantID = f.read()\n",
    "    f.close()\n",
    "    print(\"Retrieve Assistant ID: %s\"%AssistantID)\n",
    "else:\n",
    "    # Create a new Assistant\n",
    "    Assistant = Client.beta.assistants.create(\n",
    "    name=\"Assistant for \"+webfolder,\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    instructions=\"You are a school teacher answering students questions about the course material provided to you in text files. If the \\\n",
    " response is not in the text files, you can respond with 'I don't know'.\",\n",
    "    tools=[{\"type\": \"retrieval\"}],\n",
    "    # file_ids=UploadID\n",
    "    )\n",
    "    AssistantID = Assistant.id\n",
    "    # Write the Assistant ID to the file\n",
    "    f = open(DataFile, \"w\")\n",
    "    f.write(AssistantID)\n",
    "    f.close()\n",
    "    print(\"Created Assistant ID: %s\"%AssistantID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings ##\n",
    "At the time I am writing this, I have 33 blog posts under the 'general' tab on my Substack page.  Each blog post is in a separate text file.  This makes it 33 text files.  The OpenAI does not allow to attach more than 20 files.  Since I cannot attach all 33, I have to find the files that are most relevant to the question and attach only those files.  These files can be selected in a number of ways. Before we consider different options, we need to generate embeddings.  In the following cell, I do the following:\n",
    "* Load embeddings from the `npz` file if it exists\n",
    "* If there is no `npz` file, embed all files and store with metadata\n",
    "* Check if there are new text files in the TextList\n",
    "* Embed the new text files and append them to the end of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedandsave():\n",
    "    embeddings=embedpagelist(TextList)\n",
    "    metadata={\"model\":LLM, \"webfolder\":webfolder, \"textlist\":TextListFile, \"texts\":TextList}\n",
    "    np.savez(EmbeddingFile,data=embeddings, metadata=metadata)\n",
    "    return embeddings, metadata\n",
    "\n",
    "if not os.path.exists(EmbeddingFile):\n",
    "    (embeddings, metadata)=embedandsave()\n",
    "# Load embeddings from file\n",
    "with np.load(EmbeddingFile, allow_pickle=True) as data:\n",
    "    embeddings=data['data']\n",
    "    metadata=data['metadata'].item()\n",
    "\n",
    "if not metadata[\"model\"]==LLM:\n",
    "    (embeddings, metadata)=embedandsave()\n",
    "\n",
    "# Generate the liost of embedded textx that are in the npz file:\n",
    "embedded_texts=[]\n",
    "for pair in metadata[\"texts\"]:\n",
    "    embedded_texts.append(pair[1])\n",
    "\n",
    "# Now check if there are any new texts to be embedded\n",
    "update=False\n",
    "for (itext,pair) in enumerate(TextList):\n",
    "    url=pair[0]\n",
    "    texttitle=pair[1]\n",
    "    # print(\"Check '\"+texttitle+\"' --> \", end=\"\")\n",
    "    if texttitle not in embedded_texts:\n",
    "        print(\"Update \"+texttitle)\n",
    "        text=rdweb(url, None)\n",
    "        sa=sepstr(text, SEP)\n",
    "        for (isegment, s) in enumerate(sa):\n",
    "            v=get_embedding(s)\n",
    "            w=np.append(v,[itext,isegment])\n",
    "            embeddings=np.vstack((embeddings,w))\n",
    "        metadata[\"texts\"].append(pair)\n",
    "        update=True\n",
    "    # else:\n",
    "        # print(\"OK\")\n",
    "if update:\n",
    "    # Save the embeddings to a file\n",
    "    np.savez(EmbeddingFile,data=embeddings, metadata=metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Questions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpMode=0\n",
    "OpTask=[\"check\", \"answer\", \"summarise\", \"quit\"][OpMode]\n",
    "\n",
    "def checkmode():\n",
    "    global OpMode\n",
    "    OpMode=0\n",
    "    print(\"Switched to check mode\")\n",
    "    return\n",
    "def answermode():\n",
    "    global OpMode\n",
    "    OpMode=1\n",
    "    print(\"Switched to answer mode\")\n",
    "    return\n",
    "def summarisemode():\n",
    "    global OpMode\n",
    "    OpMode=2\n",
    "    print(\"Switched to summarise mode\")\n",
    "    return\n",
    "def quitmode():\n",
    "    global OpMode\n",
    "    OpMode=3\n",
    "    print(\"Quit\")\n",
    "    return\n",
    "def setoptask():\n",
    "    global OpTask\n",
    "    OpTask=[\"check\", \"answer\", \"summarise\", \"quit\"][OpMode]\n",
    "    return\n",
    "\n",
    "Specials=[\"Check\", \"Answer\", \"Summarise\", \"Quit\"]\n",
    "SpecialFunctions=[checkmode, answermode, summarisemode, quitmode]\n",
    "SpecialEmbeds=[get_embedding(s) for s in Specials]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt ##\n",
    "The following cell is the prompt I use.  It will be saved to the top of the log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are school teacher answering the questions from your pupils.    Please answer the following question from one student:\n",
       "\n",
       "'Query goes here'\n",
       "\n",
       "Use the attached text files to answer the question.       The information may not be directly available and you may have to interpret the         information to answer the query.  Try to give an answer but make sure ,         you respond with 'I don't know' when there is no answer.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q=\"Query goes here\"\n",
    "Prompt=\"You are school teacher answering the questions from your pupils.  \\\n",
    "  Please answer the following question from one student:\\n\\n'%s'\\n\\n\\\n",
    "Use the attached text files to answer the question. \\\n",
    "      The information may not be directly available and you may have to interpret the \\\n",
    "        information to answer the query.  Try to give an answer but make sure , \\\n",
    "        you respond with 'I don't know' when there is no answer.\"\n",
    "md(Prompt%q+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging our session ##\n",
    "I will store the query, the associated files, and the OpenAI response in a file.  This file will be in markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today = datetime.date.today()\n",
    "#\n",
    "Logfile=datafolder+webfolder+\"_log.md\"\n",
    "Log=open(Logfile, \"a\")\n",
    "Log.write(today.strftime(\"%B %d, %Y\")+\"\\n\\n\")\n",
    "Log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(s):\n",
    "    Log=open(Logfile, \"a\")\n",
    "    Log.write(s+\"\\n\\n\")\n",
    "    Log.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The query string ##\n",
    "In this notebook, our question is defined in the following cell.  For a new question, enter a new line with the new question content.  This process can be interactive with buttons and text entry windows as I did it in `embeddings.ipynb`.  Personally, I find it easier to do it manually by changiung the string in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query = \"Why did Duke Energy convert a gas turbine to hydrogen?\" \n",
    "# Query=\"What time of the day was the Turkish Earthquake?\"\n",
    "# Query=\"Who is Oliver Anthony?\"\n",
    "# Query=\"Does eVinci microreactor need cooling water?\"\n",
    "# Query=\"Was there an earthquake in Japan in 2011?\"\n",
    "# Query=\"Is it expensive to build earthquake resistant buildings?\"\n",
    "# Query=\"Was there an earthquake in Turkey in 2023?\"\n",
    "# Query=\"Why did so many people die in the Turkish earthquake?\"\n",
    "# Query=\"Is there going to be a war between the US and China?\"\n",
    "# Query=\"How much Local Government Debt is there in China?\"\n",
    "# Query=\"DOE Hydrogen Program\"\n",
    "# query_embedding=get_embedding(Query)\n",
    "# #\n",
    "# Log=open(Logfile, \"a\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation Mode ##\n",
    "This can do one of the following things depending on the value of the `OpMode`.  \n",
    "\n",
    "|Mode|Action|\n",
    "|:--:|----|\n",
    "|0 - Check|Do not answer. Only return the URLs of the pages that match |\n",
    "|1 - Answer|Find the best-match URLs, ask OpenAI to reply using thuse URL files|\n",
    "|2 - Summarise|Find the best-match URLs, ask OpenAI to summarise them| \n",
    "\n",
    "The default mode is `check`.  One can change the mode by entering one of the following strings (or similar) as the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|Query text|`OpTask`|\n",
       "|--|--|\n",
       "|Check|Check|\n",
       "|Answer|Answer|\n",
       "|Summarise|Summarise|\n",
       "|Quit|Quit|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table=\"|Query text|`OpTask`|\\n|--|--|\\n\"\n",
    "for s in Specials:\n",
    "    table+=\"|%s|%s|\\n\"%(s, [\"Check\", \"Answer\", \"Summarise\", \"Quit\"][Specials.index(s)])\n",
    "md(table)\n",
    "\n",
    "\n",
    "# If the query text is similar to one of the special texts,\n",
    "# then switch to the corresponding mode\n",
    "# Returns TRUE if mode is switched; FALSE otherwise\n",
    "def setmode(query_embedding):\n",
    "    for (i,s) in enumerate(Specials):\n",
    "        if cosine_similarity(query_embedding, SpecialEmbeds[i])>0.9:\n",
    "            # print(\"setmode: i=%d, s=%s\"%(i,s))\n",
    "            SpecialFunctions[i]()\n",
    "            setoptask()\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select which files to attach ##\n",
    "The easiest thing to do is to attach the top N files that are the most relevant to the query.  I will start with N=10.  The top 10 files are determined by computing the cosine-similarity of their segments to the query.\n",
    "\n",
    "The file score will be equal to the score of the closest segment in that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def findtopN(N, query_embedding):\n",
    "    global TextSimilarity, topN\n",
    "    TextSimilarity=np.ones(len(embeddings))*(-1)\n",
    "    for i in range(len(embeddings)):\n",
    "        similarity = cosine_similarity(embeddings[i][:-2], query_embedding)\n",
    "        itext=int(embeddings[i][-2])\n",
    "        if similarity>TextSimilarity[itext]:\n",
    "            TextSimilarity[itext]=similarity\n",
    "    topN=(np.argsort(TextSimilarity)[::-1])[0:N]\n",
    "    topN=topN.astype(int)\n",
    "    topN=topN.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the top N matching URLs ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prtopN():\n",
    "    s=\"**Query** : \"+Query\n",
    "    s=s+\"\\n\\nThe following are the top %d most relevant texts:\\n\\n\"%N\n",
    "    s=s+\"|itext|similarity|title|\\n|--|--|--|\\n\"\n",
    "    for i in topN:\n",
    "        s=s+\"|%d|%f|%s|\\n\"%(i,TextSimilarity[i],metadata[\"texts\"][i][1])\n",
    "    s=s+\"\\n\\nWe will attach these files to the thread.\\n\\n\"\n",
    "    md(s)\n",
    "    log(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showcheckpages(query):\n",
    "    s=\"**TOPIC** : \"+query+\"\\n\\n\"\n",
    "    for i in range(0,3):\n",
    "        s=s+\"**PAGE** : %s\\n\\n\"%(metadata[\"texts\"][topN[i]][1])\n",
    "        s=s+\"**URL** : %s\\n\\n\"%(metadata[\"texts\"][topN[i]][0])\n",
    "    md(s)\n",
    "    log(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showanswer(query):\n",
    "    msg=Client.beta.threads.messages.list(thread_id=Thread.id)\n",
    "    s=\"\\n\\n**Query** : \"+query+\"\\n\\n\"\n",
    "    s=s+\"**ANSWER** : \"+msg.data[0].content[0].text.value\n",
    "    md(s)\n",
    "    log(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Loop ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display\n",
    "\n",
    "# text = widgets.Text(\n",
    "#     value='',\n",
    "#     placeholder='What is your question ?',\n",
    "#     description='String:',\n",
    "#     disabled=False\n",
    "# )\n",
    "\n",
    "# button = widgets.Button(description=\"Click Me\")\n",
    "\n",
    "# def on_button_clicked(b):\n",
    "#     global Query\n",
    "#     global vsim\n",
    "#     Query=text.value\n",
    "#     query_embedding=get_embedding(Query)\n",
    "#     # Check if the Query is similar to one of the strings in Specials:\n",
    "#     if setmode(query_embedding):\n",
    "#         return\n",
    "#     findtopN(N, query_embedding)\n",
    "#     prtopN()\n",
    "#     if OpTask==\"check\":\n",
    "#         showcheckpages()\n",
    "#         return\n",
    "# button.on_click(on_button_clicked)\n",
    "\n",
    "# # while(OpTask!=\"quit\"):\n",
    "# #     display(text, button)\n",
    "# display(text, button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Thread ##\n",
    "Create a thread with the query and the top N relevant files as information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initmessages(query):\n",
    "   global Messages\n",
    "   usefiles=\"Use the attached text files to answer the question. \\\n",
    "        The information may not be directly available and you may have to interpret the \\\n",
    "          information to answer the query.  Try to give an answer but make sure , \\\n",
    "          you respond with 'I don't know' when there is no answer.\"\n",
    "   content=\"Please answer the following question:\\n\\n'%s'\\n\\n\\\n",
    "      \"%query+usefiles\n",
    "   Messages=[\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content,\n",
    "            \"file_ids\": [UploadID[i] for i in topN]\n",
    "          }\n",
    "        ]\n",
    "   \n",
    "\n",
    "  \n",
    "\n",
    "def thread(query):\n",
    "    global Thread, RunIDs\n",
    "    if Thread is None:\n",
    "      initmessages(query)\n",
    "      Thread = Client.beta.threads.create(messages=Messages)\n",
    "      RunIDs=[]\n",
    "    else:\n",
    "       pass\n",
    "       \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Query ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runquery():\n",
    "    run = Client.beta.threads.runs.create(\n",
    "            thread_id=Thread.id,\n",
    "            assistant_id=AssistantID,\n",
    "            instructions=\"\"\n",
    "            )\n",
    "\n",
    "    print(\"RUN #\", run.id, end=\"\")\n",
    "    RunIDs.append(run.id)\n",
    "\n",
    "    print(run.status, end=\" \")\n",
    "    while run.status != \"completed\":\n",
    "        run = Client.beta.threads.runs.retrieve(thread_id=run.thread_id, run_id=run.id)\n",
    "        print(run.status[0], end=\"\")\n",
    "        if run.status == \"completed\":\n",
    "            print(\" \\u2713\")\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "    messages = Client.beta.threads.messages.list(thread_id=Thread.id)\n",
    "    return run\n",
    "\n",
    "\n",
    "    \n",
    "# print(messages.data[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function lists the responses for all the runs on the current thread\n",
    "def show_all_messages():\n",
    "    s=\"\"\n",
    "    for i, run_id in enumerate(RunIDs):\n",
    "        steps_page=Client.beta.threads.runs.steps.list(run_id=run_id, thread_id=Thread.id)\n",
    "        message_id=steps_page.data[0].step_details.message_creation.message_id\n",
    "        message=Client.beta.threads.messages.retrieve(thread_id=Thread.id, message_id=message_id)\n",
    "        s=s+\"%02d - %s - %s\\n\\n%s\\n\\n\"%(i,Thread.id, run_id, message.content[0].text.value)\n",
    "        annotations=message.content[0].text.annotations\n",
    "        s+=\"\\n\\n%s\\n\\n\"%annotations\n",
    "    md(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Query** : WHat is the local government debt in China?\n",
       "\n",
       "The following are the top 3 most relevant texts:\n",
       "\n",
       "|itext|similarity|title|\n",
       "|--|--|--|\n",
       "|33|0.930614|Retirement: A Journey of Continued Relevance and Learning|\n",
       "|20|0.816555|Will there be a war?|\n",
       "|10|0.785798|ROGUE - Renaissance on globe with upheavals everywhere|\n",
       "\n",
       "\n",
       "We will attach these files to the thread.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**TOPIC** : WHat is the local government debt in China?\n",
       "\n",
       "**PAGE** : Retirement: A Journey of Continued Relevance and Learning\n",
       "\n",
       "**URL** : https://halimgur.substack.com/p/retirement-a-journey-of-continued\n",
       "\n",
       "**PAGE** : Will there be a war?\n",
       "\n",
       "**URL** : https://halimgur.substack.com/p/will-there-be-a-war\n",
       "\n",
       "**PAGE** : ROGUE - Renaissance on globe with upheavals everywhere\n",
       "\n",
       "**URL** : https://halimgur.substack.com/p/rogue-renaissance-on-globe-with-upheavals\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to answer mode\n",
      "Continue with the current thread\n",
      "RUN # run_9WgQkg8lHtWQKqyCvSDniP3Oqueued iiiiiiiiiiic ✓\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**Query** : WHat is the local government debt in China?\n",
       "\n",
       "**ANSWER** : In 2020, China's local government debt approached 90 trillion yuan (approximately 12.49 trillion U.S. dollars), which equated to 88% of the country's GDP at that time【11†source】."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue with the current thread\n",
      "RUN # run_N3J0FioEcthkvr3B9IMa5gK2queued iic ✓\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**Query** : This is a good answer.  Could you tell me who did this study?\n",
       "\n",
       "**ANSWER** : I don't know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue with the current thread\n",
      "RUN # run_RXMz85WRTwA5P56wQNgvSq8Vqueued iiiic ✓\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**Query** : I know the name of the Professor is in the same reference file.  Could you check that file again and tell me what that name is?\n",
       "\n",
       "**ANSWER** : In 2020, China's local government debt approached 90 trillion yuan (approximately 12.49 trillion U.S. dollars), which equated to 88% of the country's GDP at that time【10†source】."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quit\n"
     ]
    }
   ],
   "source": [
    "def chatloop():\n",
    "    global Query, Run\n",
    "    while(OpTask!=\"quit\"):\n",
    "        s=input(\"Enter command: \")\n",
    "        sembed=get_embedding(s)\n",
    "    # Check if the Query is similar to one of the strings in Specials:\n",
    "        if setmode(sembed):\n",
    "            Thread=None  # Reset the thread\n",
    "            continue\n",
    "        if s!=\"\":\n",
    "            Query=s\n",
    "            query_embedding=get_embedding(Query)\n",
    "        if OpTask==\"check\":\n",
    "            findtopN(N, query_embedding)\n",
    "            prtopN()\n",
    "            showcheckpages(Query)\n",
    "            continue\n",
    "        if OpTask==\"answer\":\n",
    "            if thread is None:\n",
    "                findtopN(N, query_embedding)\n",
    "                prtopN()\n",
    "            else:\n",
    "                print(\"Continue with the current thread\")\n",
    "            thread(Query)\n",
    "            Run=runquery()\n",
    "            showanswer(Query)\n",
    "            \n",
    "chatloop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "00 - thread_EVUsKZ9YSzqt5IHP9UL3K02b - run_9WgQkg8lHtWQKqyCvSDniP3O\n",
       "\n",
       "In 2020, China's local government debt approached 90 trillion yuan (approximately 12.49 trillion U.S. dollars), which equated to 88% of the country's GDP at that time【11†source】.\n",
       "\n",
       "\n",
       "\n",
       "[]\n",
       "\n",
       "01 - thread_EVUsKZ9YSzqt5IHP9UL3K02b - run_N3J0FioEcthkvr3B9IMa5gK2\n",
       "\n",
       "I don't know.\n",
       "\n",
       "\n",
       "\n",
       "[]\n",
       "\n",
       "02 - thread_EVUsKZ9YSzqt5IHP9UL3K02b - run_RXMz85WRTwA5P56wQNgvSq8V\n",
       "\n",
       "In 2020, China's local government debt approached 90 trillion yuan (approximately 12.49 trillion U.S. dollars), which equated to 88% of the country's GDP at that time【10†source】.\n",
       "\n",
       "\n",
       "\n",
       "[TextAnnotationFileCitation(end_index=177, file_citation=TextAnnotationFileCitationFileCitation(file_id='file-YqjgWuxwOiyGyA5gV0hbLL9C', quote=\"Meticulous analysis by Professor Li and his PhD student revealed that in 2020, China's local government debt approached 90 trillion yuan (12.49 trillion U.S. dollars), equating to 88% of the GDP at that time\"), start_index=166, text='【10†source】', type='file_citation')]\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_all_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Run(id='run_RXMz85WRTwA5P56wQNgvSq8V', assistant_id='asst_LCdnKNcUtkxVg1aDoiQ9vHCX', cancelled_at=None, completed_at=1705576778, created_at=1705576773, expires_at=None, failed_at=None, file_ids=[], instructions=\"You are a school teacher answering students questions about the course material provided to you in text files. If the  response is not in the text files, you can respond with 'I don't know'.\", last_error=None, metadata={}, model='gpt-4-1106-preview', object='thread.run', required_action=None, started_at=1705576774, status='completed', thread_id='thread_EVUsKZ9YSzqt5IHP9UL3K02b', tools=[ToolAssistantToolsRetrieval(type='retrieval')])\n",
      "0. In 2020, China's local government debt approached 90 trillion yuan (approximately 12.49 trillion U.S. dollars), which equated to 88% of the country's GDP at that time【10†source】.\n",
      "1. I don't know.\n",
      "2. In 2020, China's local government debt approached 90 trillion yuan (approximately 12.49 trillion U.S. dollars), which equated to 88% of the country's GDP at that time【11†source】.\n",
      "3. Please answer the following question:\n",
      "\n",
      "'WHat is the local government debt in China?'\n",
      "\n",
      "      Use the attached text files to answer the question.         The information may not be directly available and you may have to interpret the           information to answer the query.  Try to give an answer but make sure ,           you respond with 'I don't know' when there is no answer.\n"
     ]
    }
   ],
   "source": [
    "for (i,s) in enumerate(Client.beta.threads.messages.list(thread_id=Thread.id)):\n",
    "    print(\"%d. \"%i+s.content[0].text.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': \"No message found with id 'msg_gEkeWK9EUASTnYdLbT8EBfXh'.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mThread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmessage_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmsg_gEkeWK9EUASTnYdLbT8EBfXh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m message_content \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m      6\u001b[0m annotations \u001b[38;5;241m=\u001b[39m message_content\u001b[38;5;241m.\u001b[39mannotations\n",
      "File \u001b[0;32m~/git/probot/.venv/lib/python3.11/site-packages/openai/resources/beta/threads/messages/messages.py:124\u001b[0m, in \u001b[0;36mMessages.retrieve\u001b[0;34m(self, message_id, thread_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03mRetrieve a message.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI-Beta\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistants=v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/threads/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mthread_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/messages/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmessage_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mThreadMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/probot/.venv/lib/python3.11/site-packages/openai/_base_client.py:1006\u001b[0m, in \u001b[0;36mSyncAPIClient.get\u001b[0;34m(self, path, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;66;03m# cast is required because mypy complains about returning Any even though\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;66;03m# it understands the type variables\u001b[39;00m\n\u001b[0;32m-> 1006\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/git/probot/.venv/lib/python3.11/site-packages/openai/_base_client.py:842\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    835\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    841\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/probot/.venv/lib/python3.11/site-packages/openai/_base_client.py:885\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 885\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': \"No message found with id 'msg_gEkeWK9EUASTnYdLbT8EBfXh'.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "message = Client.beta.threads.messages.retrieve(\n",
    "  thread_id=Thread.id,\n",
    "  message_id=\"msg_gEkeWK9EUASTnYdLbT8EBfXh\"\n",
    ")\n",
    "message_content = message.content[0].text\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "\n",
    "md(message_content.value)\n",
    "# # Iterate over the annotations and add footnotes\n",
    "# for index, annotation in enumerate(annotations):\n",
    "#     # Replace the text with a footnote\n",
    "#     message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
    "\n",
    "#     # Gather citations based on annotation attributes\n",
    "#     if (file_citation := getattr(annotation, 'file_citation', None)):\n",
    "#         cited_file = Client.files.retrieve(file_citation.file_id)\n",
    "#         citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
    "#     elif (file_path := getattr(annotation, 'file_path', None)):\n",
    "#         cited_file = Client.files.retrieve(file_path.file_id)\n",
    "#         citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
    "#         # Note: File download functionality not implemented above for brevity\n",
    "\n",
    "# # Add footnotes to the end of the message before displaying to user\n",
    "# message_content.value += '\\n' + '\\n'.join(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_page=Client.beta.threads.runs.steps.list(run_id=Run.id, thread_id=Thread.id)\n",
    "for run_step in steps_page.data:\n",
    "    step_details=run_step.step_details\n",
    "    message_id = step_details.message_creation.message_id\n",
    "    print(message_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_step=steps_page.data[0]\n",
    "message_id=steps_page.data[0].step_details.message_creation.message_id\n",
    "print(message_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
