{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from ute import init_openai, read_page_list, rdweb, embedpagelist, sepstr\n",
    "from ute import SEP, get_embedding, cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Define the md() function to display markdown text\n",
    "from IPython.display import display, Markdown\n",
    "def md(s):\n",
    "    display(Markdown(s))\n",
    "\n",
    "\n",
    "(Client, LLM)=init_openai()\n",
    "webfolder=\"general\"\n",
    "datafolder=\"data\"+\"/\"+webfolder+\"/\"\n",
    "DataFile=datafolder+\"data\"+\".txt\"\n",
    "TextListFile=datafolder+webfolder+\"_textlist.txt\"\n",
    "UploadListFile=datafolder+webfolder+\"_uploadlist.txt\"\n",
    "UploadIDFile=datafolder+webfolder+\"_uploadid.txt\"\n",
    "EmbeddingFile=datafolder+webfolder+\"_embedding.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|Contents|File Name|\n",
       "|--|--|\n",
       "|Web folder name|general|\n",
       "|Data folder name|data/general/|\n",
       "|Data file name|data/general/data.txt|\n",
       "|Text list file name|data/general/general_textlist.txt|\n",
       "|Upload list file name|data/general/general_uploadlist.txt|\n",
       "|Upload ID file name|data/general/general_uploadid.txt|\n",
       "|Embedding file name|data/general/general_embedding.npz|\n",
       "|Client|<openai.OpenAI object at 0x120616a50>|\n",
       "|LLM|text-embedding-ada-002|\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Variable name|Description|\n",
       "|--|--|\n",
       "|webfolder|Web folder name|\n",
       "|datafolder|Data folder name|\n",
       "|DataFile|Data file name|\n",
       "|TextListFile|Text list file name|\n",
       "|UploadListFile|Upload list file name|\n",
       "|UploadIDFile|Upload ID file name|\n",
       "|EmbeddingFile|Embedding file name|\n",
       "|Client|OpenAI client|\n",
       "|LLM|Language model|\n",
       "UploadList|List of file names to upload|\n",
       "UploadID|List of OpenAI file IDs (created when files are uploaded)|\n",
       "TextList|List of text files (as pairs of two lines, URL and title)|\n",
       "Embedding|Embedding matrix|\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s=\"|Contents|File Name|\\n|--|--|\\n\\\n",
    "|Web folder name|\"+webfolder+\"|\\n\\\n",
    "|Data folder name|\"+datafolder+\"|\\n\\\n",
    "|Data file name|\"+DataFile+\"|\\n\\\n",
    "|Text list file name|\"+TextListFile+\"|\\n\\\n",
    "|Upload list file name|\"+UploadListFile+\"|\\n\\\n",
    "|Upload ID file name|\"+UploadIDFile+\"|\\n\\\n",
    "|Embedding file name|\"+EmbeddingFile+\"|\\n\\\n",
    "|Client|\"+str(Client)+\"|\\n\\\n",
    "|LLM|\"+str(LLM)+\"|\\n\\\n",
    "\\n\\n\"\n",
    "md(s)\n",
    "s=\"|Variable name|Description|\\n|--|--|\\n\\\n",
    "|webfolder|Web folder name|\\n\\\n",
    "|datafolder|Data folder name|\\n\\\n",
    "|DataFile|Data file name|\\n\\\n",
    "|TextListFile|Text list file name|\\n\\\n",
    "|UploadListFile|Upload list file name|\\n\\\n",
    "|UploadIDFile|Upload ID file name|\\n\\\n",
    "|EmbeddingFile|Embedding file name|\\n\\\n",
    "|Client|OpenAI client|\\n\\\n",
    "|LLM|Language model|\\n\\\n",
    "UploadList|List of file names to upload|\\n\\\n",
    "UploadID|List of OpenAI file IDs (created when files are uploaded)|\\n\\\n",
    "TextList|List of text files (as pairs of two lines, URL and title)|\\n\\\n",
    "Embedding|Embedding matrix|\\n\\\n",
    "\\n\\n\"\n",
    "md(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the text files locally and upload to OpenAI ##\n",
    "We upload the files to the file repository of the Client. They are not yet associated with any Thread.  A new thread is created after the Query is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the-requiem-for-a-dream-israels-untaken.txt because it has already been uploaded\n",
      "Skipping the-importance-of-elite-consensus.txt because it has already been uploaded\n",
      "Skipping openai-astounds-us-again.txt because it has already been uploaded\n",
      "Skipping why-did-elon-musk-buy-twitter.txt because it has already been uploaded\n",
      "Skipping elon-musk-the-spaceman.txt because it has already been uploaded\n",
      "Skipping let-us-talk-about-elon-musk.txt because it has already been uploaded\n",
      "Skipping despicable-acts-part-2.txt because it has already been uploaded\n",
      "Skipping despicable-deeds.txt because it has already been uploaded\n",
      "Skipping rogue-age-and-climate-change-unpredictable.txt because it has already been uploaded\n",
      "Skipping rogue-age-accessory-1-population.txt because it has already been uploaded\n",
      "Skipping rogue-renaissance-on-globe-with-upheavals.txt because it has already been uploaded\n",
      "Skipping the-great-stagnation-ends-but-for.txt because it has already been uploaded\n",
      "Skipping when-the-rivers-run-dry.txt because it has already been uploaded\n",
      "Skipping the-voice-referendum-in-australia.txt because it has already been uploaded\n",
      "Skipping conspiracy-theories-part-2.txt because it has already been uploaded\n",
      "Skipping conspiracy-theories-part-1.txt because it has already been uploaded\n",
      "Skipping how-many-more-ruins-in-great-britain.txt because it has already been uploaded\n",
      "Skipping while-watching-utopia-on-abc.txt because it has already been uploaded\n",
      "Skipping lying-oracles-and-the-anyone-and.txt because it has already been uploaded\n",
      "Skipping the-one-thing-necessary-for-the-triumph.txt because it has already been uploaded\n",
      "Skipping will-there-be-a-war.txt because it has already been uploaded\n",
      "Skipping how-do-we-select-our-information.txt because it has already been uploaded\n",
      "Skipping do-we-know-why-we-know-what-we-know.txt because it has already been uploaded\n",
      "Skipping does-layering-affect-heat-loss.txt because it has already been uploaded\n",
      "Skipping elections-in-turkey.txt because it has already been uploaded\n",
      "Skipping monster-wave-soon-to-hit-your-shore.txt because it has already been uploaded\n",
      "Skipping try-counting-carbons-not-calories.txt because it has already been uploaded\n",
      "Skipping bridge-fixture-for-five-players.txt because it has already been uploaded\n",
      "Skipping the-attainment-of-happiness.txt because it has already been uploaded\n",
      "Skipping competent-intelligence-is-here-will.txt because it has already been uploaded\n",
      "Skipping the-prodigal-bird-returns.txt because it has already been uploaded\n",
      "Skipping earthquake-thoughts-and-facts.txt because it has already been uploaded\n",
      "Skipping why-were-so-many-buildings-destroyed.txt because it has already been uploaded\n",
      "No new files uploaded\n",
      "No new IDs uploaded\n"
     ]
    }
   ],
   "source": [
    "# The following function reads the names of the URLs already uploaded to OpenAI\n",
    "def read_upload_list(UploadListFile):\n",
    "    UploadList=[]\n",
    "    if os.path.exists(UploadListFile):\n",
    "        with open(UploadListFile, 'r') as f:\n",
    "            UploadList = f.read().splitlines()\n",
    "    return UploadList\n",
    "\n",
    "UploadList=read_upload_list(UploadListFile)\n",
    "\n",
    "# The following function reads the IDs of the URLs already uploaded to OpenAI\n",
    "def read_upload_id(UploadIDFile):\n",
    "    UploadID=[]\n",
    "    if os.path.exists(UploadIDFile):\n",
    "        with open(UploadIDFile, 'r') as f:\n",
    "            UploadID = f.read().splitlines()\n",
    "    return UploadID\n",
    "\n",
    "UploadID=read_upload_id(UploadIDFile)\n",
    "#\n",
    "# The following function extracts a file name from the URL.  The file name is\n",
    "# used to name the text file that will be uploaded to OpenAI\n",
    "# The file name is also used to check if the file has already been uploaded\n",
    "# Use the last part of the URL as the file name\n",
    "def get_file_name(url):\n",
    "    url_parts=url.split(\"/\")\n",
    "    filename=url_parts[-1]\n",
    "    return filename\n",
    "\n",
    "# Read the URL addresses to be read into text files from the file TextListFile\n",
    "# Ignore the URLs that have already been uploaded to OpenAI\n",
    "\n",
    "TextList=read_page_list(TextListFile)\n",
    "NewUploads=[]\n",
    "NewUploadIDs=[]\n",
    "for (itext,pair) in enumerate(TextList):\n",
    "    (url,title)=pair\n",
    "    txtfile=get_file_name(url)+\".txt\"\n",
    "    if txtfile in UploadList:\n",
    "        print(\"Skipping %s because it has already been uploaded\"%txtfile)\n",
    "        continue\n",
    "    # Rad the text from the URL address and save into the text file\n",
    "    rdweb(url, datafolder+txtfile)\n",
    "    print(\"Uploading %s\"%txtfile, end=\"\")\n",
    "    File = Client.files.create(\n",
    "        file=open(datafolder+txtfile, \"rb\"),\n",
    "        purpose='assistants'\n",
    ")    \n",
    "    print(\" --- Uploaded\")\n",
    "    time.sleep(1)\n",
    "    NewUploads.append(txtfile)\n",
    "    NewUploadIDs.append(File.id)\n",
    "with open(UploadListFile, 'a') as f:\n",
    "    for item in NewUploads:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "if len(NewUploads)>0:\n",
    "    print(\"Updated %s (Names of the Uploaded Files)\"%UploadListFile)\n",
    "else:\n",
    "    print(\"No new files uploaded\")\n",
    "# Append the new IDs to the file UploadIDFile\n",
    "with open(UploadIDFile, 'a') as f:\n",
    "    for item in NewUploadIDs:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "if len(NewUploadIDs)>0:\n",
    "    print(\"Updated %s (OpenAI IDs of the uploaded files)\"%UploadIDFile)\n",
    "else:\n",
    "    print(\"No new IDs uploaded\")\n",
    "#\n",
    "UploadID=UploadID+NewUploadIDs\n",
    "UploadList=UploadList+NewUploads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Assistant ID from file or create new ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve Assistant ID: asst_LCdnKNcUtkxVg1aDoiQ9vHCX\n"
     ]
    }
   ],
   "source": [
    "# Check if the DataFile exists\n",
    "if os.path.isfile(DataFile):\n",
    "    # Read the Assistant ID from the file\n",
    "    f = open(DataFile, \"r\")\n",
    "    AssistantID = f.read()\n",
    "    f.close()\n",
    "    print(\"Retrieve Assistant ID: %s\"%AssistantID)\n",
    "else:\n",
    "    # Create a new Assistant\n",
    "    Assistant = Client.beta.assistants.create(\n",
    "    name=\"Assistant for \"+webfolder,\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    instructions=\"You are a school teacher answering students questions about the course material provided to you in text files. If the \\\n",
    " response is not in the text files, you can respond with 'I don't know'.\",\n",
    "    tools=[{\"type\": \"retrieval\"}],\n",
    "    # file_ids=UploadID\n",
    "    )\n",
    "    AssistantID = Assistant.id\n",
    "    # Write the Assistant ID to the file\n",
    "    f = open(DataFile, \"w\")\n",
    "    f.write(AssistantID)\n",
    "    f.close()\n",
    "    print(\"Created Assistant ID: %s\"%AssistantID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings ##\n",
    "At the time I am writing this, I have 33 blog posts under the 'general' tab on my Substack page.  Each blog post is in a separate text file.  This makes it 33 text files.  The OpenAI does not allow to attach more than 20 files.  Since I cannot attach all 33, I have to find the files that are most relevant to the question and attach only those files.  These files can be selected in a number of ways. Before we consider different options, we need to generate embeddings.  In the following cell, I do the following:\n",
    "* Load embeddings from the `npz` file if it exists\n",
    "* If there is no `npz` file, embed all files and store with metadata\n",
    "* Check if there are new text files in the TextList\n",
    "* Embed the new text files and append them to the end of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedandsave():\n",
    "    embeddings=embedpagelist(TextList)\n",
    "    metadata={\"model\":LLM, \"webfolder\":webfolder, \"textlist\":TextListFile, \"texts\":TextList}\n",
    "    np.savez(EmbeddingFile,data=embeddings, metadata=metadata)\n",
    "    return embeddings, metadata\n",
    "\n",
    "if not os.path.exists(EmbeddingFile):\n",
    "    (embeddings, metadata)=embedandsave()\n",
    "# Load embeddings from file\n",
    "with np.load(EmbeddingFile, allow_pickle=True) as data:\n",
    "    embeddings=data['data']\n",
    "    metadata=data['metadata'].item()\n",
    "\n",
    "if not metadata[\"model\"]==LLM:\n",
    "    (embeddings, metadata)=embedandsave()\n",
    "\n",
    "# Generate the liost of embedded textx that are in the npz file:\n",
    "embedded_texts=[]\n",
    "for pair in metadata[\"texts\"]:\n",
    "    embedded_texts.append(pair[1])\n",
    "\n",
    "# Now check if there are any new texts to be embedded\n",
    "update=False\n",
    "for (itext,pair) in enumerate(TextList):\n",
    "    url=pair[0]\n",
    "    texttitle=pair[1]\n",
    "    # print(\"Check '\"+texttitle+\"' --> \", end=\"\")\n",
    "    if texttitle not in embedded_texts:\n",
    "        print(\"Update \"+texttitle)\n",
    "        text=rdweb(url, None)\n",
    "        sa=sepstr(text, SEP)\n",
    "        for (isegment, s) in enumerate(sa):\n",
    "            v=get_embedding(s)\n",
    "            w=np.append(v,[itext,isegment])\n",
    "            embeddings=np.vstack((embeddings,w))\n",
    "        metadata[\"texts\"].append(pair)\n",
    "        update=True\n",
    "    # else:\n",
    "        # print(\"OK\")\n",
    "if update:\n",
    "    # Save the embeddings to a file\n",
    "    np.savez(EmbeddingFile,data=embeddings, metadata=metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Questions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpMode=0\n",
    "OpTask=[\"check\", \"answer\", \"summarise\"][OpMode]\n",
    "\n",
    "def checkmode():\n",
    "    global OpMode\n",
    "    OpMode=0\n",
    "    print(\"Switched to check mode\")\n",
    "    return\n",
    "def answermode():\n",
    "    global OpMode\n",
    "    OpMode=1\n",
    "    print(\"Switched to answer mode\")\n",
    "    return\n",
    "def summarisemode():\n",
    "    global OpMode\n",
    "    OpMode=2\n",
    "    print(\"Switched to summarise mode\")\n",
    "    return\n",
    "\n",
    "Specials=[\"Only provide the URL\", \"Prepare to answer questions\", \"Summarise the pages\"]\n",
    "SpecialFunctions=[checkmode, answermode, summarisemode]\n",
    "SpecialEmbeds=[get_embedding(s) for s in Specials]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt ##\n",
    "The following cell is the prompt I use.  It will be saved to the top of the log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are school teacher answering the questions from your pupils.    Please answer the following question from one student:\n",
       "\n",
       "'Query goes here'\n",
       "\n",
       "Use the attached text files to answer the question.       The information may not be directly available and you may have to interpret the         information to answer the query.  Try to give an answer but make sure ,         you respond with 'I don't know' when there is no answer.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q=\"Query goes here\"\n",
    "Prompt=\"You are school teacher answering the questions from your pupils.  \\\n",
    "  Please answer the following question from one student:\\n\\n'%s'\\n\\n\\\n",
    "Use the attached text files to answer the question. \\\n",
    "      The information may not be directly available and you may have to interpret the \\\n",
    "        information to answer the query.  Try to give an answer but make sure , \\\n",
    "        you respond with 'I don't know' when there is no answer.\"\n",
    "md(Prompt%q+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging our session ##\n",
    "I will store the query, the associated files, and the OpenAI response in a file.  This file will be in markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logfile=datafolder+webfolder+\"_log.md\"\n",
    "Log=open(Logfile, \"a\")\n",
    "Log.write(\"**Prompt**\\n\\n\"+Prompt+\"\\n\\n\")\n",
    "Log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The query string ##\n",
    "In this notebook, our question is defined in the following cell.  For a new question, enter a new line with the new question content.  This process can be interactive with buttons and text entry windows as I did it in `embeddings.ipynb`.  Personally, I find it easier to do it manually by changiung the string in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = \"Why did Duke Energy convert a gas turbine to hydrogen?\" \n",
    "Query=\"What time of the day was the Turkish Earthquake?\"\n",
    "Query=\"Who is Oliver Anthony?\"\n",
    "Query=\"Does eVinci microreactor need cooling water?\"\n",
    "Query=\"Was there an earthquake in Japan in 2011?\"\n",
    "Query=\"Is it expensive to build earthquake resistant buildings?\"\n",
    "Query=\"Was there an earthquake in Turkey in 2023?\"\n",
    "Query=\"Why did so many people die in the Turkish earthquake?\"\n",
    "Query=\"Is there going to be a war between the US and China?\"\n",
    "Query=\"How much Local Government Debt is there in China?\"\n",
    "Query=\"DOE Hydrogen Program\"\n",
    "query_embedding=get_embedding(Query)\n",
    "#\n",
    "Log=open(Logfile, \"a\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation Mode ##\n",
    "This can do one of the following things deopendng on the value of the `OpMode`.  \n",
    "\n",
    "|Mode|Action|\n",
    "|:--:|----|\n",
    "|0 - Check|Do not answer. Only return the URLs of the pages that match |\n",
    "|1 - Answer|Find the best-match URLs, ask OpenAI to reply using thuse URL files|\n",
    "|2 - Summarise|Find the best-match URLs, ask OpenAI to summarise them| \n",
    "\n",
    "The default mode is `check`.  One can change the mode by entering one of the following strings (or similar) as the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|Query text|For Mode|\n",
       "|--|--|\n",
       "|Only provide the URL|Check|\n",
       "|Prepare to answer questions|Answer|\n",
       "|Summarise the pages|Summarise|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table=\"|Query text|For Mode|\\n|--|--|\\n\"\n",
    "for s in Specials:\n",
    "    table+=\"|%s|%s|\\n\"%(s, [\"Check\", \"Answer\", \"Summarise\"][Specials.index(s)])\n",
    "md(table)\n",
    "md(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the query, change `OpMode` is necessary ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if the Query is similar to one of the strings in Specials:\n",
    "for (i,s) in enumerate(Specials):\n",
    "    if cosine_similarity(query_embedding, SpecialEmbeds[i])>0.9:\n",
    "        SpecialFunctions[i]()\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select which files to attach ##\n",
    "The easiest thing to do is to attach the top N files that are the most relevant to the query.  I will start with N=10.  The top 10 files are determined by computing the cosine-similarity of their segments to the query.\n",
    "\n",
    "The file score will be equal to the score of the closest segment in that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=3\n",
    "textsimilarity=np.ones(len(embeddings))*(-1)\n",
    "for i in range(len(embeddings)):\n",
    "    similarity = cosine_similarity(embeddings[i][:-2], query_embedding)\n",
    "    itext=int(embeddings[i][-2])\n",
    "    if similarity>textsimilarity[itext]:\n",
    "        textsimilarity[itext]=similarity\n",
    "topN=(np.argsort(textsimilarity)[::-1])[0:N]\n",
    "topN=topN.astype(int)\n",
    "topN=topN.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Query** : DOE Hydrogen Program\n",
       "\n",
       "The following are the top 3 most relevant texts:\n",
       "\n",
       "|itext|similarity|title|\n",
       "|--|--|--|\n",
       "|6|0.821517|Despicable Acts - Part 2|\n",
       "|1|0.800706|The importance of elite consensus|\n",
       "|9|0.780822|ROGUE Age Accessory #1 - Population|\n",
       "\n",
       "\n",
       "We will attach these files to the thread.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s=\"**Query** : \"+Query\n",
    "s=s+\"\\n\\nThe following are the top %d most relevant texts:\\n\\n\"%N\n",
    "s=s+\"|itext|similarity|title|\\n|--|--|--|\\n\"\n",
    "for i in topN:\n",
    "    s=s+\"|%d|%f|%s|\\n\"%(i,textsimilarity[i],metadata[\"texts\"][i][1])\n",
    "s=s+\"\\n\\nWe will attach these files to the thread.\\n\\n\"\n",
    "md(s)\n",
    "#Log.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Thread ##\n",
    "Create a thread with the query and the top N relevant files as information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def thread():\n",
    "    nextquestion=Query\n",
    "    content=\"Please answer the following question:\\n\\n'%s'\\n\\n\\\n",
    "    Use the attached text files to answer the question. \\\n",
    "      The information may not be directly available and you may have to interpret the \\\n",
    "        information to answer the query.  Try to give an answer but make sure , \\\n",
    "        you respond with 'I don't know' when there is no answer.\"%nextquestion\n",
    "    Thread = Client.beta.threads.create(\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": content,\n",
    "          \"file_ids\": [UploadID[i] for i in topN]\n",
    "        }\n",
    "    ]\n",
    "    )\n",
    "    return Thread\n",
    "\n",
    "\n",
    "if OpTask==\"answer\":\n",
    "    Thread=thread()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Query ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runquery():\n",
    "    run = Client.beta.threads.runs.create(\n",
    "            thread_id=Thread.id,\n",
    "            assistant_id=AssistantID,\n",
    "            instructions=\"\"\n",
    "            )\n",
    "\n",
    "    print(\"RUN STARTED.  run id: \", run.id)\n",
    "\n",
    "    print(run.status, end=\" \")\n",
    "    while run.status != \"completed\":\n",
    "        run = Client.beta.threads.runs.retrieve(thread_id=run.thread_id, run_id=run.id)\n",
    "        print(run.status, end=\" \")\n",
    "        if run.status == \"completed\":\n",
    "            print(\"Completed\")\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "    messages = Client.beta.threads.messages.list(thread_id=Thread.id)\n",
    "    return messages\n",
    "\n",
    "if OpTask==\"answer\":\n",
    "    messages=runquery()\n",
    "# print(messages.data[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the OpenAI Answer ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "** TOPIC ** : DOE Hydrogen Program\n",
       "\n",
       "**PAGE** : Despicable Acts - Part 2\n",
       "\n",
       "**URL** : https://halimgur.substack.com/p/despicable-acts-part-2\n",
       "\n",
       "**PAGE** : The importance of elite consensus\n",
       "\n",
       "**URL** : https://halimgur.substack.com/p/the-importance-of-elite-consensus\n",
       "\n",
       "**PAGE** : ROGUE Age Accessory #1 - Population\n",
       "\n",
       "**URL** : https://halimgur.substack.com/p/rogue-age-accessory-1-population\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if OpTask==\"check\":\n",
    "    s=\"** TOPIC ** : \"+Query+\"\\n\\n\"\n",
    "    for i in range(0,3):\n",
    "        s=s+\"**PAGE** : %s\\n\\n\"%(metadata[\"texts\"][topN[i]][1])\n",
    "        s=s+\"**URL** : %s\\n\\n\"%(metadata[\"texts\"][topN[i]][0])\n",
    "elif OpTask==\"answer\":\n",
    "    s=\"\\n\\n**Query** : \"+Query+\"\\n\\n\"\n",
    "    s=s+\"**ANSWER** : \"+messages.data[0].content[0].text.value\n",
    "md(s)\n",
    "md(\"\\n\\n\")\n",
    "Log.write(s)\n",
    "Log.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
